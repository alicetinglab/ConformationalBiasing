{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CB with Frame2seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../utilities/\")\n",
    "\n",
    "from cbutils import (\n",
    "    aa_code,\n",
    "    get_chain_seq,\n",
    "    get_chain_seq_for_scoring,\n",
    "    make_consensus_sequence,\n",
    "    setup_aligner,\n",
    "    alignment_to_mapping,\n",
    "    mapping_to_sequence,\n",
    "    add_scaled_outputs,\n",
    ")\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from frame2seq import Frame2seqRunner\n",
    "from frame2seq.utils import residue_constants\n",
    "from frame2seq.utils.util import get_neg_pll\n",
    "from frame2seq.utils.pdb2input import get_inference_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select structures and chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdbs = {\n",
    "    \"open\": \"../pdbs/lpla/3a7r.pdb\",\n",
    "    \"closed\": \"../pdbs/lpla/1x2g.pdb\",\n",
    "}\n",
    "\n",
    "chains = {\n",
    "    'open':\"A\",\n",
    "    'closed':\"A\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Align sequences, make mutants, and score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up frame2seq scoring function\n",
    "def frame2seq_score(runner, pdb_file: str, chain_id: str, input_seqs: list[str]):\n",
    "    \"\"\"\n",
    "    Calculates the pseudo-log-likelihood (PLL) scores for a list of input sequences\n",
    "    given a structure using a Frame2seq model ensemble.\n",
    "\n",
    "    Args:\n",
    "        runner: Frame2seqRunner.\n",
    "            An initialized Frame2seqRunner object containing the ensemble models.\n",
    "        pdb_file: str\n",
    "            Path to a PDB file containing the desired protein structure.\n",
    "        chain_id: str\n",
    "            Chain identifier (e.g., 'A') corresponding to the chain of interest in the PDB file.\n",
    "        input_seqs: list of str\n",
    "            List of amino acid sequences to be evaluated against the structure. Must be length matched.\n",
    "\n",
    "    Returns:\n",
    "        scores: list of float\n",
    "            List of negative PLL scores, one for each input sequence. Higher (less negative)\n",
    "            values indicate sequences more compatible with the structure.\n",
    "    \"\"\"\n",
    "    # Get structure-based input tensors for inference\n",
    "    seq_mask, backbone_seq_tokenized, X = get_inference_inputs(pdb_file, chain_id)\n",
    "\n",
    "    # Decode backbone sequence from tokenized integer representation\n",
    "    backbone_seq = [\n",
    "        residue_constants.ID_TO_AA[int(i)] for i in backbone_seq_tokenized[0]\n",
    "    ]\n",
    "\n",
    "    # Convert backbone sequence to one-hot encoding using standard AA to ID mapping\n",
    "    backbone_seq_onehot = residue_constants.sequence_to_onehot(\n",
    "        sequence=backbone_seq,\n",
    "        mapping=residue_constants.AA_TO_ID,\n",
    "    )\n",
    "\n",
    "    # Convert one-hot numpy array to torch tensor and move to runner device\n",
    "    backbone_seq_onehot = (\n",
    "        torch.from_numpy(backbone_seq_onehot).float().unsqueeze(0).to(runner.device)\n",
    "    )\n",
    "    # Mask all positions in sequence by setting them to 'X' (unknown amino acid)\n",
    "    backbone_seq_onehot = torch.zeros_like(backbone_seq_onehot)\n",
    "    backbone_seq_onehot[:, :, 20] = 1  # 20 = 'X', mask all positions\n",
    "\n",
    "    scores = []  # list to collect scores for each input sequence\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Run all three ensemble models to get amino acid probabilities\n",
    "        aaprobs1 = runner.models[0].forward(X, seq_mask, backbone_seq_onehot)\n",
    "        aaprobs2 = runner.models[1].forward(X, seq_mask, backbone_seq_onehot)\n",
    "        aaprobs3 = runner.models[2].forward(X, seq_mask, backbone_seq_onehot)\n",
    "\n",
    "        # Average logits from ensemble models\n",
    "        aaprobs = (aaprobs1 + aaprobs2 + aaprobs3) / 3  # ensemble model predictions\n",
    "\n",
    "        # Apply softmax to obtain amino acid probability distributions\n",
    "        aaprobs = torch.nn.functional.softmax(aaprobs, dim=-1)\n",
    "\n",
    "        # Only keep probabilities at valid sequence mask positions\n",
    "        aaprobs = aaprobs[seq_mask]\n",
    "\n",
    "        # Convert each input sequence to tensor of residue IDs on the runner device\n",
    "        input_seqs = [\n",
    "            torch.tensor([residue_constants.AA_TO_ID[aa] for aa in seq])\n",
    "            .long()\n",
    "            .to(runner.device)\n",
    "            for seq in input_seqs\n",
    "        ]\n",
    "\n",
    "        # For each input sequence, calculate and collect the negative PLL score (log-likelihood under model)\n",
    "        for sample in tqdm(range(len(input_seqs))):\n",
    "            input_seq_i = input_seqs[sample]\n",
    "            _neg_pll, avg_neg_pll = get_neg_pll(aaprobs, input_seq_i)\n",
    "            scores.append(-1 * avg_neg_pll)  # multiply by -1 to return PLL\n",
    "\n",
    "    return scores  # return list of scores, one per input sequence\n",
    "\n",
    "# get sequences and align\n",
    "seqs = {pdb: get_chain_seq(pdbs[pdb], chains[pdb]) for pdb in pdbs}\n",
    "scoring_seqs = {pdb: get_chain_seq_for_scoring(pdbs[pdb], chains[pdb]) for pdb in pdbs}\n",
    "con_seq = make_consensus_sequence(list(seqs.values()))\n",
    "\n",
    "aligner = setup_aligner()\n",
    "alignments = {pdb: aligner.align(con_seq, seq)[0] for pdb, seq in scoring_seqs.items()}\n",
    "\n",
    "mappings = {\n",
    "    pdb: alignment_to_mapping(alignment) for pdb, alignment in alignments.items()\n",
    "}\n",
    "\n",
    "# make mutants\n",
    "muts = []\n",
    "mut_seqs = []\n",
    "for i, aa in enumerate(con_seq):\n",
    "    for aa_new in aa_code:\n",
    "        if aa_new != aa:\n",
    "            mut_seqs.append(con_seq[:i] + aa_new + con_seq[i + 1 :])\n",
    "            muts.append(f\"{aa}{i+1}{aa_new}\")\n",
    "\n",
    "#load frame2seq runner object\n",
    "runner = Frame2seqRunner()\n",
    "\n",
    "#score sequences\n",
    "output_data = pd.DataFrame({\"mut\": muts, \"seq\": mut_seqs})\n",
    "\n",
    "for structure in pdbs:\n",
    "    output_seqs = []\n",
    "\n",
    "    wt_seq = mapping_to_sequence(con_seq, scoring_seqs[structure], mappings[structure])\n",
    "\n",
    "    with open(f\"{structure}.fasta\", \"w\") as f:\n",
    "        for mut_seq, mut in zip(mut_seqs, muts):\n",
    "            mapped_seq = mapping_to_sequence(\n",
    "                mut_seq, scoring_seqs[structure], mappings[structure]\n",
    "            )\n",
    "            output_seqs.append(mapped_seq)\n",
    "\n",
    "    wt_score = frame2seq_score(runner, pdbs[structure], chains[structure], [wt_seq])[0]\n",
    "    outs = frame2seq_score(runner, pdbs[structure], chains[structure], output_seqs)\n",
    "\n",
    "    output_data[f\"frame2seq_{structure}\"] = [x - wt_score for x in outs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"frame2seq\"\n",
    "frac_mutants = 0.05\n",
    "\n",
    "# scale columns and calculate bias\n",
    "add_scaled_outputs(output_data, model, state1_col=\"open\", state2_col=\"closed\")\n",
    "\n",
    "# filter mutants by low scores\n",
    "output_data = output_data.dropna(subset=[f\"{model}_state1_bias\"]).sort_values(\n",
    "    by=f\"{model}_state1_bias\", ascending=False\n",
    ")\n",
    "passing_mutants = output_data[\n",
    "    (output_data[f\"{model}_state1_scaled\"] > 0)\n",
    "    | (output_data[f\"{model}_state2_scaled\"] > 0)\n",
    "]\n",
    "nonpassing = output_data[\n",
    "    ~(\n",
    "        (output_data[f\"{model}_state1_scaled\"] > 0)\n",
    "        | (output_data[f\"{model}_state2_scaled\"] > 0)\n",
    "    )\n",
    "]\n",
    "\n",
    "# take top n biased mutants in each direction\n",
    "n_mutants_passing_filter = len(\n",
    "    output_data[\n",
    "        (output_data[f\"{model}_state1_scaled\"] > 0)\n",
    "        | (output_data[f\"{model}_state2_scaled\"] > 0)\n",
    "    ]\n",
    ")\n",
    "n_biased = round((frac_mutants / 2) * n_mutants_passing_filter)\n",
    "\n",
    "state1_biased, neutral, state2_biased = (\n",
    "    passing_mutants[:n_biased],\n",
    "    passing_mutants[n_biased:-n_biased],\n",
    "    passing_mutants[-n_biased:],\n",
    ")\n",
    "\n",
    "s1_set, s2_set, neutral_set, nonpassing_set = (\n",
    "    set(state1_biased[\"mut\"]),\n",
    "    set(state2_biased[\"mut\"]),\n",
    "    set(neutral[\"mut\"]),\n",
    "    set(nonpassing[\"mut\"]),\n",
    ")\n",
    "\n",
    "assignments = []\n",
    "for m in output_data[\"mut\"]:\n",
    "    if m in set(state1_biased[\"mut\"]):\n",
    "        assignment = \"state1\"\n",
    "    elif m in set(state2_biased[\"mut\"]):\n",
    "        assignment = \"state2\"\n",
    "    elif m in neutral_set:\n",
    "        assignment = \"neutral\"\n",
    "    elif m in set(nonpassing[\"mut\"]):\n",
    "        assignment = \"low\"\n",
    "    else:\n",
    "        assignment = None\n",
    "\n",
    "    assignments.append(assignment)\n",
    "\n",
    "# label mutants\n",
    "output_data[f\"{model}_assignment\"] = assignments\n",
    "\n",
    "cmap = {\"state1\": \"red\", \"state2\": \"blue\", \"neutral\": \"grey\", \"low\": \"lightgrey\"}\n",
    "\n",
    "passing = output_data[output_data[f\"{model}_assignment\"] != \"low\"]\n",
    "nonpassing = output_data[output_data[f\"{model}_assignment\"] == \"low\"]\n",
    "\n",
    "state1_cutoff = output_data[output_data[f\"{model}_assignment\"] == \"state1\"][\n",
    "    f\"{model}_state1_bias\"\n",
    "].min()\n",
    "state2_cutoff = output_data[output_data[f\"{model}_assignment\"] == \"state2\"][\n",
    "    f\"{model}_state2_bias\"\n",
    "].min()\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title(\"Conformational Design Mutants (Top 5% mutants)\")\n",
    "\n",
    "plt.scatter(\n",
    "    passing[f\"{model}_state1_scaled\"],\n",
    "    passing[f\"{model}_state2_scaled\"],\n",
    "    marker=\"o\",\n",
    "    alpha=0.7,\n",
    "    edgecolor=\"black\",\n",
    "    c=[cmap[x] for x in passing[f\"{model}_assignment\"]],\n",
    ")\n",
    "plt.scatter(\n",
    "    nonpassing[f\"{model}_state1_scaled\"],\n",
    "    nonpassing[f\"{model}_state2_scaled\"],\n",
    "    marker=\"o\",\n",
    "    alpha=0.25,\n",
    "    edgecolor=\"black\",\n",
    "    c=[cmap[x] for x in nonpassing[f\"{model}_assignment\"]],\n",
    ")\n",
    "\n",
    "# set limits to be equal on both axes\n",
    "xmin, xmax = plt.xlim()\n",
    "ymin, ymax = plt.ylim()\n",
    "\n",
    "umin, umax = min(xmin, ymin), max(xmax, ymax)\n",
    "plt.xlim(umin, umax)\n",
    "plt.ylim(umin, umax)\n",
    "\n",
    "# show cutoffs\n",
    "plt.plot([umin, 0], [0, 0], color=\"black\")\n",
    "plt.plot([0, 0], [umin, 0], color=\"black\")\n",
    "\n",
    "plt.plot([-state2_cutoff, umax - state2_cutoff], [0, umax], color=\"black\")\n",
    "plt.plot([0, umax], [-state1_cutoff, umax - state1_cutoff], color=\"black\")\n",
    "\n",
    "plt.xlabel(f\"State 1 {model} Score\")\n",
    "plt.ylabel(f\"State 2 {model} Score\")\n",
    "\n",
    "# label each section\n",
    "text_offset = 0.1\n",
    "plt.text(\n",
    "    umax - text_offset,\n",
    "    umax - text_offset,\n",
    "    \"Neutral Mutants\",\n",
    "    horizontalalignment=\"right\",\n",
    "    verticalalignment=\"top\",\n",
    ")\n",
    "plt.text(\n",
    "    umax - text_offset,\n",
    "    umin + text_offset,\n",
    "    \"State 1 Bias Predicted Mutants\",\n",
    "    horizontalalignment=\"right\",\n",
    "    verticalalignment=\"bottom\",\n",
    ")\n",
    "plt.text(\n",
    "    umin + text_offset,\n",
    "    umax - text_offset,\n",
    "    \"State 2 Bias Predicted Mutants\",\n",
    "    horizontalalignment=\"left\",\n",
    "    verticalalignment=\"top\",\n",
    ")\n",
    "plt.text(\n",
    "    umin + text_offset,\n",
    "    umin + text_offset,\n",
    "    \"Low Scoring Mutants\",\n",
    "    horizontalalignment=\"left\",\n",
    "    verticalalignment=\"bottom\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cbtest2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
