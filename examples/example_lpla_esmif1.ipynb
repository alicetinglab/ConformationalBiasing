{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CB with ESM-IF1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../utilities/\")\n",
    "\n",
    "from cbutils import (\n",
    "    aa_code,\n",
    "    make_consensus_sequence,\n",
    "    setup_aligner,\n",
    "    alignment_to_mapping,\n",
    "    mapping_to_sequence,\n",
    "    AA_ALPHABET,\n",
    "    add_scaled_outputs,\n",
    ")\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "import esm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from esm.inverse_folding.util import CoordBatchConverter\n",
    "from scipy.special import softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select structures and chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set path to input pdbs\n",
    "pdbs = {\n",
    "    \"open\": \"../pdbs/lpla/3a7r.pdb\",\n",
    "    \"closed\": \"../pdbs/lpla/1x2g.pdb\",\n",
    "}\n",
    "\n",
    "#specify chains for each structure\n",
    "chains = {\n",
    "    \"open\": \"A\",\n",
    "    \"closed\": \"A\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Align sequences, make mutants, and score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load model and alphabet\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model, alphabet = esm.pretrained.esm_if1_gvp4_t16_142M_UR50()\n",
    "\n",
    "model = model.cuda()\n",
    "model = model.eval()\n",
    "\n",
    "#set up scoring functions\n",
    "def get_esmif_aaprobs(model, alphabet, coords, seq):\n",
    "    \"\"\"\n",
    "    Compute amino acid probabilities for a given sequence and structure using ESM-IF1.\n",
    "\n",
    "    Args:\n",
    "        model: The ESM-IF1 model.\n",
    "        alphabet: The ESM alphabet object.\n",
    "        coords: Numpy array of backbone coordinates for the structure.\n",
    "        seq: Amino acid sequence (string).\n",
    "\n",
    "    Returns:\n",
    "        aa_probs: DataFrame of amino acid probabilities (rows: AAs, columns: positions).\n",
    "    \"\"\"\n",
    "    # Get device from model\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    # Prepare batch for the model\n",
    "    batch_converter = CoordBatchConverter(alphabet)\n",
    "    batch = [(coords, None, seq)]\n",
    "    coords, confidence, strs, tokens, padding_mask = batch_converter(\n",
    "        batch, device=device\n",
    "    )\n",
    "\n",
    "    # Prepare input tokens for the model\n",
    "    prev_output_tokens = tokens[:, :-1].to(device)\n",
    "    target = tokens[:, 1:]\n",
    "    target_padding_mask = target == alphabet.padding_idx\n",
    "\n",
    "    # Forward pass through the model\n",
    "    logits, _ = model.forward(coords, padding_mask, confidence, prev_output_tokens)\n",
    "    logits = logits.cpu().detach().numpy().squeeze()\n",
    "\n",
    "    # Compute softmax probabilities and format as DataFrame\n",
    "    aa_probs = pd.DataFrame(\n",
    "        softmax(logits), \n",
    "        index=list(alphabet.to_dict().keys())\n",
    "    )\n",
    "    # Restrict to standard amino acids\n",
    "    aa_probs = aa_probs.loc[[aa for aa in AA_ALPHABET], :]\n",
    "    # Normalize probabilities at each position\n",
    "    aa_probs = aa_probs / aa_probs.sum(axis=0)\n",
    "\n",
    "    return aa_probs\n",
    "\n",
    "\n",
    "def get_esmif_score(seq, model, alphabet, coords):\n",
    "    \"\"\"\n",
    "    Compute the log-probability score of a sequence given structure using ESM-IF1.\n",
    "\n",
    "    Args:\n",
    "        seq: Amino acid sequence (string).\n",
    "        model: The ESM-IF1 model.\n",
    "        alphabet: The ESM alphabet object.\n",
    "        coords: Numpy array of backbone coordinates for the structure.\n",
    "\n",
    "    Returns:\n",
    "        score: Log-probability score (float).\n",
    "    \"\"\"\n",
    "    # Get amino acid probabilities for the sequence\n",
    "    aa_probs = get_esmif_aaprobs(model, alphabet, coords, seq)\n",
    "    score = 0.0\n",
    "    # Sum log-probabilities for each residue in the sequence\n",
    "    for idx, aa in enumerate(seq):\n",
    "        score += np.log(aa_probs.loc[aa, idx])\n",
    "    return score\n",
    "\n",
    "seqs = {}\n",
    "coords = {}\n",
    "for pdb in pdbs:\n",
    "    structure = esm.inverse_folding.util.load_structure(pdbs[pdb], chains[pdb])\n",
    "    coord, native_seq = esm.inverse_folding.util.extract_coords_from_structure(\n",
    "        structure\n",
    "    )\n",
    "    seqs[pdb] = native_seq\n",
    "    coords[pdb] = coord\n",
    "con_seq = make_consensus_sequence(list(seqs.values()))\n",
    "\n",
    "aligner = setup_aligner()\n",
    "alignments = {pdb: aligner.align(con_seq, seq)[0] for pdb, seq in seqs.items()}\n",
    "\n",
    "mappings = {\n",
    "    pdb: alignment_to_mapping(alignment) for pdb, alignment in alignments.items()\n",
    "}\n",
    "\n",
    "muts = []\n",
    "mut_seqs = []\n",
    "for i, aa in enumerate(con_seq):\n",
    "    for aa_new in aa_code:\n",
    "        if aa_new != aa:\n",
    "            mut_seqs.append(con_seq[:i] + aa_new + con_seq[i + 1 :])\n",
    "            muts.append(f\"{aa}{i+1}{aa_new}\")\n",
    "\n",
    "output_data = pd.DataFrame({\"mut\": muts, \"seq\": mut_seqs})\n",
    "\n",
    "for structure in pdbs:\n",
    "    output_scores = []\n",
    "\n",
    "    wt_seq = mapping_to_sequence(con_seq, seqs[structure], mappings[structure])\n",
    "    wt_score = get_esmif_score(wt_seq, model, alphabet, coords[structure])\n",
    "\n",
    "    for mut_seq in tqdm(mut_seqs):\n",
    "        mapped_seq = mapping_to_sequence(mut_seq, seqs[structure], mappings[structure])\n",
    "        score = get_esmif_score(mapped_seq, model, alphabet, coords[structure])\n",
    "        output_scores.append(score - wt_score)\n",
    "\n",
    "    output_data[\"esmif1_\" + structure] = output_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"esmif1\"\n",
    "frac_mutants = 0.05\n",
    "\n",
    "# scale columns and calculate bias\n",
    "add_scaled_outputs(output_data, model, state1_col=\"open\", state2_col=\"closed\")\n",
    "\n",
    "# filter mutants by low scores\n",
    "output_data = output_data.dropna(subset=[f\"{model}_state1_bias\"]).sort_values(\n",
    "    by=f\"{model}_state1_bias\", ascending=False\n",
    ")\n",
    "passing_mutants = output_data[\n",
    "    (output_data[f\"{model}_state1_scaled\"] > 0)\n",
    "    | (output_data[f\"{model}_state2_scaled\"] > 0)\n",
    "]\n",
    "nonpassing = output_data[\n",
    "    ~(\n",
    "        (output_data[f\"{model}_state1_scaled\"] > 0)\n",
    "        | (output_data[f\"{model}_state2_scaled\"] > 0)\n",
    "    )\n",
    "]\n",
    "\n",
    "# take top n biased mutants in each direction\n",
    "n_mutants_passing_filter = len(\n",
    "    output_data[\n",
    "        (output_data[f\"{model}_state1_scaled\"] > 0)\n",
    "        | (output_data[f\"{model}_state2_scaled\"] > 0)\n",
    "    ]\n",
    ")\n",
    "n_biased = round((frac_mutants / 2) * n_mutants_passing_filter)\n",
    "\n",
    "state1_biased, neutral, state2_biased = (\n",
    "    passing_mutants[:n_biased],\n",
    "    passing_mutants[n_biased:-n_biased],\n",
    "    passing_mutants[-n_biased:],\n",
    ")\n",
    "\n",
    "s1_set, s2_set, neutral_set, nonpassing_set = (\n",
    "    set(state1_biased[\"mut\"]),\n",
    "    set(state2_biased[\"mut\"]),\n",
    "    set(neutral[\"mut\"]),\n",
    "    set(nonpassing[\"mut\"]),\n",
    ")\n",
    "\n",
    "assignments = []\n",
    "for m in output_data[\"mut\"]:\n",
    "    if m in set(state1_biased[\"mut\"]):\n",
    "        assignment = \"state1\"\n",
    "    elif m in set(state2_biased[\"mut\"]):\n",
    "        assignment = \"state2\"\n",
    "    elif m in neutral_set:\n",
    "        assignment = \"neutral\"\n",
    "    elif m in set(nonpassing[\"mut\"]):\n",
    "        assignment = \"low\"\n",
    "    else:\n",
    "        assignment = None\n",
    "\n",
    "    assignments.append(assignment)\n",
    "\n",
    "# label mutants\n",
    "output_data[f\"{model}_assignment\"] = assignments\n",
    "\n",
    "cmap = {\"state1\": \"red\", \"state2\": \"blue\", \"neutral\": \"grey\", \"low\": \"lightgrey\"}\n",
    "\n",
    "passing = output_data[output_data[f\"{model}_assignment\"] != \"low\"]\n",
    "nonpassing = output_data[output_data[f\"{model}_assignment\"] == \"low\"]\n",
    "\n",
    "state1_cutoff = output_data[output_data[f\"{model}_assignment\"] == \"state1\"][\n",
    "    f\"{model}_state1_bias\"\n",
    "].min()\n",
    "state2_cutoff = output_data[output_data[f\"{model}_assignment\"] == \"state2\"][\n",
    "    f\"{model}_state2_bias\"\n",
    "].min()\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title(\"Conformational Design Mutants (Top 5% mutants)\")\n",
    "\n",
    "plt.scatter(\n",
    "    passing[f\"{model}_state1_scaled\"],\n",
    "    passing[f\"{model}_state2_scaled\"],\n",
    "    marker=\"o\",\n",
    "    alpha=0.7,\n",
    "    edgecolor=\"black\",\n",
    "    c=[cmap[x] for x in passing[f\"{model}_assignment\"]],\n",
    ")\n",
    "plt.scatter(\n",
    "    nonpassing[f\"{model}_state1_scaled\"],\n",
    "    nonpassing[f\"{model}_state2_scaled\"],\n",
    "    marker=\"o\",\n",
    "    alpha=0.25,\n",
    "    edgecolor=\"black\",\n",
    "    c=[cmap[x] for x in nonpassing[f\"{model}_assignment\"]],\n",
    ")\n",
    "\n",
    "# set limits to be equal on both axes\n",
    "xmin, xmax = plt.xlim()\n",
    "ymin, ymax = plt.ylim()\n",
    "\n",
    "umin, umax = min(xmin, ymin), max(xmax, ymax)\n",
    "plt.xlim(umin, umax)\n",
    "plt.ylim(umin, umax)\n",
    "\n",
    "# show cutoffs\n",
    "plt.plot([umin, 0], [0, 0], color=\"black\")\n",
    "plt.plot([0, 0], [umin, 0], color=\"black\")\n",
    "\n",
    "plt.plot([-state2_cutoff, umax - state2_cutoff], [0, umax], color=\"black\")\n",
    "plt.plot([0, umax], [-state1_cutoff, umax - state1_cutoff], color=\"black\")\n",
    "\n",
    "plt.xlabel(f\"State 1 {model} Score\")\n",
    "plt.ylabel(f\"State 2 {model} Score\")\n",
    "\n",
    "# label each section\n",
    "text_offset = 0.1\n",
    "plt.text(\n",
    "    umax - text_offset,\n",
    "    umax - text_offset,\n",
    "    \"Neutral Mutants\",\n",
    "    horizontalalignment=\"right\",\n",
    "    verticalalignment=\"top\",\n",
    ")\n",
    "plt.text(\n",
    "    umax - text_offset,\n",
    "    umin + text_offset,\n",
    "    \"State 1 Bias Predicted Mutants\",\n",
    "    horizontalalignment=\"right\",\n",
    "    verticalalignment=\"bottom\",\n",
    ")\n",
    "plt.text(\n",
    "    umin + text_offset,\n",
    "    umax - text_offset,\n",
    "    \"State 2 Bias Predicted Mutants\",\n",
    "    horizontalalignment=\"left\",\n",
    "    verticalalignment=\"top\",\n",
    ")\n",
    "plt.text(\n",
    "    umin + text_offset,\n",
    "    umin + text_offset,\n",
    "    \"Low Scoring Mutants\",\n",
    "    horizontalalignment=\"left\",\n",
    "    verticalalignment=\"bottom\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cbtest2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
