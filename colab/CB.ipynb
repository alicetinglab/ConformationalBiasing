{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVmFMidn965N"
      },
      "source": [
        "# **Conformational Biasing! (CB)**\n",
        "*Updated: 10/17/2025 by Andrew Xue*\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/alicetinglab/ConformationalBiasing/main/images/CB.jpg\" align=\"center\" alt=\"drawing\" width=\"800\">\n",
        "\n",
        "This notebook implements a computational pipeline for designing protein variants with altered conformational state preferences, based on contrastive scoring using inverse folding models across alternative structures of a protein. Running this notebook with default inputs will generate predictions for LplA, a protein we engineered for promiscuous and site-specific labeling using CB.\n",
        "\n",
        "For more information, see below:\n",
        "- [*Pre-print: Computational design of conformation-biasing mutations to alter protein functions*](https://www.biorxiv.org/content/10.1101/2025.05.03.652001v1)\n",
        "- [Github Repository](https://github.com/alicetinglab/ConformationalBiasing)\n",
        "\n",
        "**Usage Tracking**: This notebook uses a small counter to track how many users have run it. This helps us understand the reach of the tool and improve on its functionality. The only data recorded is the number of times it has been run - no other data is saved.\n",
        "\n",
        "## Overview and Inputs\n",
        "The notebook takes two protein structures representing different conformational states of a protein and:\n",
        "1. Aligns the two protein structures/sequences to generate a consensus sequence\n",
        "2. Generates all possible single-point mutations (user can upload additional sequences to score)\n",
        "3. Evaluates variant sequences for biasing effect towards either conformation\n",
        "\n",
        "**Input Structure Requirements and Suggestions:**\n",
        "1. Accepted inputs:\n",
        "    - PDB ID (4-letter)\n",
        "    - AlphaFold DB accession\n",
        "    - Custom PDB or mmCIF (uploaded by user)\n",
        "2. Structures should be for the same protein (not orthologs, ideally minimal point mutations)\n",
        "3. Structures should have good and overlapping coverage of the backbone, and similar resolution\n",
        "4. Any uncommon amino acids (e.g. PTR, MSE) will be converted to the closest standard amino acid. Unrecognized amino acids may result in errors/no predictions for that position.\n",
        "5. AlphaFold/predicted structures seem to be acceptable, and can be used to generate a structure in a given conformation by templating on an ortholog in that structure\n",
        "\n",
        "**Other optional inputs:**\n",
        "1. Additional sequences (e.g. double mutants, designed variants of proteins) to be scored, in fasta format\n",
        "2. Reference sequence. The sequence will be automatically extracted from structure files, but in case both files are missing the same region or the user wants to enforce a certain AA numbering. Reference sequence may be augmented after input if it does not totally cover both PDBs (e.g. truncated at the C-terminus)\n",
        "## Models and Code\n",
        "We implement CB with four different inverse folding models:\n",
        "- **ProteinMPNN** (Default): [Reference](https://www.science.org/doi/10.1126/science.add2187), [Code](https://github.com/dauparas/ProteinMPNN)\n",
        "- **Frame2Seq**: [Reference](https://www.mlsb.io/papers_2023/Frame2seq_structure-conditioned_masked_language_modeling_for_protein_sequence_design.pdf), [Code](https://github.com/dakpinaroglu/Frame2seq)\n",
        "- **ThermoMPNN**: [Reference](https://www.pnas.org/doi/10.1073/pnas.2314853121), [Code](https://github.com/Kuhlman-Lab/ThermoMPNN)\n",
        "- **ESM-IF1**: [Reference](https://proceedings.mlr.press/v162/hsu22a/hsu22a.pdf), [Code](https://github.com/facebookresearch/esm)\n",
        "\n",
        "Pipeline prioritizes running ProteinMPNN, as it is the most extensively validated in our manuscript. Additional models can be run at the user's preference.\n",
        "\n",
        "Model codebases were modified slightly in order to ensure compatibility and ease of use. No weights were modified, nor any other functional changes. Running ProteinMPNN is enabled by code from [ColabDesign](https://github.com/sokrypton/ColabDesign) (thank you to [solab](https://www.solab.org/)!). Changes to other repositories can be found at the following links: [Frame2Seq](https://github.com/andrewxue98/Frame2seq), [ThermoMPNN](https://github.com/andrewxue98/ThermoMPNN), and [ESM-IF1](https://github.com/andrewxue98/esm)\n",
        "\n",
        "Overlapping predictions between all models seem to be high confidence biasing mutations (see CB manuscript, Figure S11).\n",
        "\n",
        "## Output\n",
        "\n",
        "The pipeline produces a CSV file containing:\n",
        "- Generated mutations and variant sequences\n",
        "- Scores from each model for both conformational states\n",
        "- Predicted conformational bias classifications for each model\n",
        "\n",
        "## Estimated Runtime\n",
        "Our method is lightweight and can quickly evaluate thousands of point mutations. For saturation mutagenesis, the runtime does scale exponentially for larger proteins, which may require additional considerations. For a protein of size ~38kDa/340 amino acids, model evaluation time for various GPUs is approximately:\n",
        "- **ProteinMPNN**: 3 min *(T4)*, 1 min *(L40)*, 1 min *(A100)*\n",
        "- **Frame2Seq**: &lt;1min *(T4, L40, A100)*\n",
        "- **ThermoMPNN**: &lt;1min *(T4, L40, A100)*\n",
        "- **ESM-IF1**: 1 hour *(T4)*, 30 min *(L40)*, 20 min *(A100)*\n",
        "\n",
        "## Issues, Troubleshooting, and Limitations\n",
        "**Issues:**\n",
        "1. Please first read through the troubleshooting guide below and see if any of those steps are able to resolve your issues.\n",
        "2. If that fails, please open an issue [**here**](https://github.com/alicetinglab/ConformationalBiasing/issues) and include detailed information about the issue you encounter and your inputs.\n",
        "3. For any non-breaking bugs you encounter, please open an issue as well (thanks!).\n",
        "\n",
        "**Troubleshooting:**\n",
        "1. Check if you have these common issues:\n",
        "    - *Uploaded duplicate structures or extremely similar structures:*\n",
        "        - CB scatter plot will show points with almost perfect correlation.\n",
        "        - Please ensure you're uploading two different conformational states of a protein. We see good results even on subtle shifts so if points are almost perfectly correlated, it's likely that the conformational difference captured in the structure is not meaningful.\n",
        "    - *Uploaded structures of different protein:*\n",
        "        - Very poor alignment on alignment chart, almost no mutant sequences generated.\n",
        "        - Please check your structures in the structure viewer below.\n",
        "    - *Uploaded structures of protein orthologs:*\n",
        "        - Poor/spotty alignment on alignment chart, many points in alignment missing, fewer mutant sequences generated\n",
        "        - Upload a structure with a matching sequence (could be AlphaFold generated using original structure as a template)\n",
        "    - *Poor overlap of resolved backbones:*\n",
        "        - Would show as non-overlapping alignments on alignment chart\n",
        "        - Some structures are only solved on part of the backbone. This pipeline only works on regions where the structure is solved in both conformations. Please generate/find structures where the backbone overlap is good.\n",
        "    - *Selected incorrect chains:*\n",
        "        - Possibly poor alignment on alignment chart, fewer mutant sequences generated.\n",
        "        - Please check chains in structure viewer below and make sure you're selecting the correct chain\n",
        "2. Check structure files in pyMOL or similar:\n",
        "    - Ensure chains are not named abnormally\n",
        "    - Check for unnatural amino acids (some are handled by default, but not all)\n",
        "    - Check for missing atoms\n",
        "    - Sometimes [PDBFixer](https://github.com/openmm/pdbfixer) can help resolve these issues\n",
        "\n",
        "**Current Limitations:**\n",
        "- ESM-IF1: runtime is orders of magnitude slower than other models\n",
        "- ThermoMPNN: currently only setup to handle single mutants\n",
        "- This notebook supports single chain scoring only currently\n",
        "- Sensitivity across all variants: we think that our predicted-biased variants have a good hit-rate (see LplA data from manuscript). However, we have not exhaustively characterized false-negatives; it is likely that many mutations we predict as \"neutral\" could have some effect on conformational occupancy.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tnwjNa-IWMSA",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Setup and Dependencies\n",
        "# @markdown This cell installs required packages (colabdesign, gemmi, py3Dmol) and defines core utility functions for:\n",
        "# @markdown 1. Structure handling (PDB/mmCIF parsing, chain extraction)\n",
        "# @markdown 2. Sequence manipulation (residue mapping, alignment)\n",
        "# @markdown 3. Visualization (structure viewing, sequence display)\n",
        "# @markdown 4. File management (unique IDs, file conversion)\n",
        "import os\n",
        "\n",
        "# JAX option: don't preallocate VRAM\n",
        "os.environ[\"XLA_PYTHON_CLIENT_PREALLOCATE\"] = \"false\"\n",
        "\n",
        "try:\n",
        "    import colabdesign\n",
        "except:\n",
        "    os.system(\"pip -q install git+https://github.com/sokrypton/ColabDesign.git@v1.1.1\")\n",
        "\n",
        "try:\n",
        "    import gemmi\n",
        "except:\n",
        "    os.system(\"pip -q install gemmi\")\n",
        "\n",
        "try:\n",
        "    import py3Dmol\n",
        "except:\n",
        "    os.system(\"pip -q install py3Dmol\")\n",
        "\n",
        "import uuid\n",
        "import gemmi\n",
        "import py3Dmol\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "from Bio.Align import PairwiseAligner\n",
        "from IPython.display import HTML\n",
        "from pathlib import Path\n",
        "from typing import Optional, Union, Sequence, List, Dict, Tuple\n",
        "from scipy.special import softmax\n",
        "\n",
        "from google.colab import files\n",
        "from google.colab import data_table\n",
        "data_table.enable_dataframe_formatter()\n",
        "\n",
        "from colabdesign.mpnn import mk_mpnn_model, clear_mem\n",
        "\n",
        "##############################################\n",
        "# PDB Upload and Conversion Utilities\n",
        "##############################################\n",
        "def cif_to_pdb(\n",
        "    in_path: Union[str, Path],\n",
        "    out_path: Optional[Union[str, Path]] = None,\n",
        "    *,\n",
        "    assembly: Optional[str] = None,\n",
        "    # assembly options: None (asymmetric unit), \"1\" (use that bioassembly ID),\n",
        "    # \"bio\" (first defined assembly), or \"unit_cell\" (expand ASU to P1)\n",
        "    naming: str = \"short\",          # 'short' | 'add_number' | 'dup'\n",
        "    merge_dist: float = 0.2,        # merge overlapping atoms in assembly; set 0 to skip\n",
        "    keep_spacegroup: bool = False,  # keep crystal SG when transforming to assembly\n",
        "    minimal_headers: bool = False   # write only minimal PDB headers\n",
        ") -> Path:\n",
        "    \"\"\"\n",
        "    Convert an mmCIF (.cif or .cif.gz) to PDB using Gemmi.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    in_path : str | Path\n",
        "        Input mmCIF file (supports .cif or .cif.gz).\n",
        "    out_path : str | Path | None\n",
        "        Output PDB path. Defaults to input basename with .pdb extension.\n",
        "    assembly : str | None\n",
        "        None -> write asymmetric unit.\n",
        "        \"unit_cell\" -> expand ASU to full P1 unit cell.\n",
        "        \"bio\" -> use the first biological assembly defined in the file.\n",
        "        \"<ID>\" -> use that specific assembly ID (e.g., \"1\", \"2\", ...).\n",
        "    naming : str\n",
        "        How to name copied chains when expanding assemblies:\n",
        "        'short' (best for PDB), 'add_number', or 'dup'.\n",
        "    merge_dist : float\n",
        "        Distance threshold for merging identical overlapping atoms after expansion.\n",
        "        Use 0.0 to disable merging.\n",
        "    keep_spacegroup : bool\n",
        "        Keep crystal space group instead of switching to non-crystal P1 when expanding.\n",
        "    minimal_headers : bool\n",
        "        If True, write a minimal PDB (CRYST1 + coordinates, no extras).\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Path\n",
        "        Path to the written .pdb file.\n",
        "    \"\"\"\n",
        "    in_path = Path(in_path)\n",
        "\n",
        "    # Default output name: handle .cif and .cif.gz\n",
        "    if out_path is None:\n",
        "        # strip .gz if present, then replace .cif with .pdb\n",
        "        out_path = in_path.with_suffix(\"\") if in_path.suffix == \".gz\" else in_path\n",
        "        out_path = Path(out_path).with_suffix(\".pdb\")\n",
        "    out_path = Path(out_path)\n",
        "\n",
        "    # Read structure (Gemmi auto-handles .gz and format detection)\n",
        "    st = gemmi.read_structure(str(in_path))\n",
        "    st.setup_entities()  # recommended for consistent behavior on lightly-annotated files\n",
        "\n",
        "    # Expand to biological assembly or unit cell if requested\n",
        "    if assembly is not None:\n",
        "        name_map = {\n",
        "            \"short\": gemmi.HowToNameCopiedChain.Short,\n",
        "            \"add_number\": gemmi.HowToNameCopiedChain.AddNumber,\n",
        "            \"dup\": gemmi.HowToNameCopiedChain.Dup,\n",
        "        }\n",
        "        how = name_map.get(naming.lower(), gemmi.HowToNameCopiedChain.Short)\n",
        "\n",
        "        if assembly == \"unit_cell\":\n",
        "            st.transform_to_assembly(\"unit_cell\", how=how, merge_dist=merge_dist, keep_spacegroup=keep_spacegroup)\n",
        "        else:\n",
        "            # choose an assembly ID\n",
        "            if assembly in (\"bio\", \"biological\", \"biological_assembly\"):\n",
        "                if not st.assemblies:\n",
        "                    raise ValueError(\"No biological assemblies are defined in this file.\")\n",
        "                assembly_id = st.assemblies[0].name\n",
        "            else:\n",
        "                assembly_id = str(assembly)\n",
        "\n",
        "            st.transform_to_assembly(assembly_name=assembly_id, how=how,\n",
        "                                     merge_dist=merge_dist, keep_spacegroup=keep_spacegroup)\n",
        "\n",
        "        # Make sure serials are tidy after expansion\n",
        "        st.assign_serial_numbers(numbered_ter=False)\n",
        "\n",
        "    # Write PDB\n",
        "    write_opts = gemmi.PdbWriteOptions(minimal=minimal_headers)\n",
        "    st.write_pdb(str(out_path), write_opts)\n",
        "    return out_path\n",
        "\n",
        "def rand_id() -> str:\n",
        "    \"\"\"\n",
        "    Generate a unique random identifier for file naming.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    str\n",
        "        A 32-character hexadecimal string that is unique for each call.\n",
        "        Useful for creating unique filenames for uploaded files.\n",
        "    \"\"\"\n",
        "    return uuid.uuid4().hex\n",
        "\n",
        "\n",
        "def get_pdb(pdb_code=\"\", upload_message=None, upload_out_path=None):\n",
        "    \"\"\"\n",
        "    Acquire a PDB file in various ways:\n",
        "    - If pdb_code is blank or None, prompt for file upload via Colab (UI) and convert to PDB if needed.\n",
        "    - If pdb_code is a filename, return it if it exists.\n",
        "    - If pdb_code is a 4-character string, download from RCSB.\n",
        "    - Otherwise, download from AlphaFold.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    pdb_code : str, optional\n",
        "        PDB code, file name, or AlphaFold code. If empty or None, prompts for upload. (default: \"\")\n",
        "    upload_message : str, optional\n",
        "        HTML message shown when prompting for upload. (default: None)\n",
        "    upload_out_path : str, optional\n",
        "        Output path for the uploaded or converted file. Generate random if not provided. (default: None)\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    str\n",
        "        Path to the PDB file.\n",
        "\n",
        "    Raises\n",
        "    ------\n",
        "    ValueError\n",
        "        If PDB code is invalid or file download fails\n",
        "    FileNotFoundError\n",
        "        If local file not found or upload fails\n",
        "    RuntimeError\n",
        "        If file conversion fails\n",
        "    \"\"\"\n",
        "    if pdb_code is None or pdb_code == \"\":\n",
        "        try:\n",
        "            if upload_message is not None:\n",
        "                display(HTML(f\"<h3>{upload_message}</h3>\"))\n",
        "\n",
        "            if upload_out_path is None:\n",
        "                out_path = rand_id() + \".pdb\"\n",
        "            else:\n",
        "                out_path = upload_out_path\n",
        "\n",
        "            upload_dict = files.upload()\n",
        "            if not upload_dict:\n",
        "                raise FileNotFoundError(\"No file was uploaded\")\n",
        "\n",
        "            valid_extensions = (\".pdb\", \".cif\", \".cif.gz\")\n",
        "            first_upload_key = None\n",
        "            for k in upload_dict:\n",
        "                if k.endswith(valid_extensions):\n",
        "                    first_upload_key = k\n",
        "                    break\n",
        "\n",
        "            if first_upload_key is None:\n",
        "                raise ValueError(\n",
        "                    f\"Uploaded file must end with one of: {valid_extensions}\"\n",
        "                )\n",
        "\n",
        "            try:\n",
        "                if first_upload_key.endswith((\".cif\", \".cif.gz\")):\n",
        "                    cif_to_pdb(first_upload_key, out_path)\n",
        "                else:\n",
        "                    pdb_string = upload_dict[first_upload_key]\n",
        "                    with open(out_path, \"wb\") as out:\n",
        "                        out.write(pdb_string)\n",
        "            except Exception as e:\n",
        "                raise RuntimeError(f\"Failed to convert/save uploaded file: {str(e)}\")\n",
        "\n",
        "            # Clean up uploaded files\n",
        "            for file in upload_dict:\n",
        "                if os.path.exists(file):\n",
        "                    os.remove(file)\n",
        "\n",
        "            return out_path\n",
        "\n",
        "        except Exception as e:\n",
        "            # Clean up any partial files\n",
        "            if \"out_path\" in locals() and os.path.exists(out_path):\n",
        "                os.remove(out_path)\n",
        "            raise\n",
        "\n",
        "    elif os.path.isfile(pdb_code):\n",
        "        return pdb_code\n",
        "\n",
        "    elif len(pdb_code) == 4:\n",
        "        # Download from RCSB PDB\n",
        "        pdb_file = f\"{pdb_code}.pdb\"\n",
        "        download_url = f\"https://files.rcsb.org/view/{pdb_file}\"\n",
        "\n",
        "        response = os.system(f\"wget -qnc {download_url}\")\n",
        "        if response != 0 or not os.path.exists(pdb_file):\n",
        "            raise ValueError(f\"Failed to download PDB {pdb_code} from RCSB\")\n",
        "\n",
        "        return pdb_file\n",
        "\n",
        "    else:\n",
        "        # Try AlphaFold DB\n",
        "        af_file = f\"AF-{pdb_code}-F1-model_v3.pdb\"\n",
        "        download_url = f\"https://alphafold.ebi.ac.uk/files/{af_file}\"\n",
        "\n",
        "        response = os.system(f\"wget -qnc {download_url}\")\n",
        "        if response != 0 or not os.path.exists(af_file):\n",
        "            raise ValueError(\n",
        "                f\"Failed to download structure {pdb_code} from AlphaFold DB\"\n",
        "            )\n",
        "\n",
        "        return af_file\n",
        "##############################################\n",
        "# Amino Acid Constants and Conversion Functions\n",
        "##############################################\n",
        "\n",
        "AA_ALPHABET = \"ARNDCQEGHILKMFPSTWYV\"\n",
        "\n",
        "# Standard 20 amino acids (3-letter)\n",
        "STANDARD_AA3 = {\n",
        "    \"ALA\",\n",
        "    \"ARG\",\n",
        "    \"ASN\",\n",
        "    \"ASP\",\n",
        "    \"CYS\",\n",
        "    \"GLN\",\n",
        "    \"GLU\",\n",
        "    \"GLY\",\n",
        "    \"HIS\",\n",
        "    \"ILE\",\n",
        "    \"LEU\",\n",
        "    \"LYS\",\n",
        "    \"MET\",\n",
        "    \"PHE\",\n",
        "    \"PRO\",\n",
        "    \"SER\",\n",
        "    \"THR\",\n",
        "    \"TRP\",\n",
        "    \"TYR\",\n",
        "    \"VAL\",\n",
        "}\n",
        "\n",
        "# Common PDB residue variants & PTMs mapped to canonical residues.\n",
        "# Extend as needed for your datasets.\n",
        "NONCANONICAL_TO_CANONICAL = {\n",
        "    # Selenium substitutions\n",
        "    \"MSE\": \"MET\",  # Selenomethionine\n",
        "    \"SEC\": \"CYS\",  # Selenocysteine (U) → treat as CYS\n",
        "    \"CSE\": \"CYS\",  # Alternate code seen for Sec\n",
        "    # Phosphorylations\n",
        "    \"PTR\": \"TYR\",  # Phosphotyrosine\n",
        "    \"TPO\": \"THR\",  # Phosphothreonine\n",
        "    \"SEP\": \"SER\",  # Phosphoserine\n",
        "    # Cys oxidations / variants\n",
        "    \"CSO\": \"CYS\",  # Cys sulfinic acid\n",
        "    \"CSD\": \"CYS\",\n",
        "    \"CME\": \"CYS\",\n",
        "    \"CYM\": \"CYS\",  # Deprotonated cysteine\n",
        "    \"CYX\": \"CYS\",  # Disulfide-bonded form\n",
        "    # His protonation states\n",
        "    \"HID\": \"HIS\",\n",
        "    \"HIE\": \"HIS\",\n",
        "    \"HIP\": \"HIS\",\n",
        "    # Acid/base tautomers\n",
        "    \"ASH\": \"ASP\",\n",
        "    \"GLH\": \"GLU\",\n",
        "    \"LYN\": \"LYS\",\n",
        "    # Hydroxyproline\n",
        "    \"HYP\": \"PRO\",\n",
        "    # Methyl-lysines (common ones)\n",
        "    \"MLY\": \"LYS\",\n",
        "    \"M3L\": \"LYS\",\n",
        "    # Rare noncanonical → closest canonical\n",
        "    \"PYL\": \"LYS\",  # Pyrrolysine (O) → treat as Lys\n",
        "    # Catch-alls sometimes seen for modified Asn/Gln\n",
        "    \"MEN\": \"ASN\",\n",
        "    \"MEQ\": \"GLN\",\n",
        "}\n",
        "\n",
        "ONE_LETTER = {\n",
        "    \"ALA\": \"A\",\n",
        "    \"ARG\": \"R\",\n",
        "    \"ASN\": \"N\",\n",
        "    \"ASP\": \"D\",\n",
        "    \"CYS\": \"C\",\n",
        "    \"GLN\": \"Q\",\n",
        "    \"GLU\": \"E\",\n",
        "    \"GLY\": \"G\",\n",
        "    \"HIS\": \"H\",\n",
        "    \"ILE\": \"I\",\n",
        "    \"LEU\": \"L\",\n",
        "    \"LYS\": \"K\",\n",
        "    \"MET\": \"M\",\n",
        "    \"PHE\": \"F\",\n",
        "    \"PRO\": \"P\",\n",
        "    \"SER\": \"S\",\n",
        "    \"THR\": \"T\",\n",
        "    \"TRP\": \"W\",\n",
        "    \"TYR\": \"Y\",\n",
        "    \"VAL\": \"V\",\n",
        "}\n",
        "\n",
        "\n",
        "def canonicalize_resname(resname3: str) -> str:\n",
        "    \"\"\"\n",
        "    Convert a 3-letter residue name to its canonical form.\n",
        "\n",
        "    Handles common non-canonical residues (e.g., modified residues, PTMs)\n",
        "    by mapping them to their parent canonical amino acid. For example:\n",
        "    - MSE (selenomethionine) -> MET\n",
        "    - PTR (phosphotyrosine) -> TYR\n",
        "    - CSE (selenocysteine) -> CYS\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    resname3 : str\n",
        "        3-letter residue code to canonicalize\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    str\n",
        "        Canonical 3-letter residue code. If the input is unknown and not\n",
        "        mapped to a canonical residue, returns the uppercased input.\n",
        "\n",
        "    Notes\n",
        "    -----\n",
        "    The complete mapping of non-canonical to canonical residues is defined\n",
        "    in the NONCANONICAL_TO_CANONICAL dictionary.\n",
        "    \"\"\"\n",
        "    r = (resname3 or \"\").upper()\n",
        "    if r in STANDARD_AA3:\n",
        "        return r\n",
        "    if r in NONCANONICAL_TO_CANONICAL:\n",
        "        return NONCANONICAL_TO_CANONICAL[r]\n",
        "    return r  # leave as-is; caller can choose to skip if not in STANDARD_AA3\n",
        "\n",
        "##############################################\n",
        "# PDB helper functions\n",
        "##############################################\n",
        "def list_chain_residues(\n",
        "    structure_path: str,\n",
        "    chain_id: str,\n",
        "    *,\n",
        "    include_noncanonical=False,\n",
        "    model_index: int = 0,\n",
        ") -> List[Dict[str, Union[str, int]]]:\n",
        "    \"\"\"\n",
        "    Read a PDB/mmCIF with gemmi and list residues in a specified chain.\n",
        "    - Converts common noncanonical/modified residues to their canonical amino acids.\n",
        "    - Positions include seq number and insertion code.\n",
        "    - By default, skips residues that are not canonical or mapped to canonical.\n",
        "      Set include_noncanonical=True to keep any unmapped residues.\n",
        "\n",
        "    Returns a list of dicts like:\n",
        "      {\"resname_raw\": \"PTR\", \"resname\": \"TYR\", \"one_letter\": \"Y\",\n",
        "       \"seqnum\": 123, \"icode\": \"\", \"chain\": \"A\"}\n",
        "    \"\"\"\n",
        "    st = gemmi.read_structure(structure_path)\n",
        "    if model_index < 0 or model_index >= len(st):\n",
        "        raise IndexError(\n",
        "            f\"Model index {model_index} out of range (structure has {len(st)} models).\"\n",
        "        )\n",
        "\n",
        "    model = st[model_index]\n",
        "\n",
        "    # Try to find the exact chain ID; gemmi stores polymer chains in model.walk_chains()\n",
        "    target_chain = None\n",
        "    for ch in model:\n",
        "        if ch.name == chain_id:\n",
        "            target_chain = ch\n",
        "            break\n",
        "    if target_chain is None:\n",
        "        # Also check entity-based iteration (e.g., for mmCIF)\n",
        "        for ch in model.chains:\n",
        "            if ch.name == chain_id:\n",
        "                target_chain = ch\n",
        "                break\n",
        "    if target_chain is None:\n",
        "        raise ValueError(\n",
        "            f\"Chain '{chain_id}' not found in model {model_index} of {structure_path!r}.\"\n",
        "        )\n",
        "\n",
        "    out: List[Dict[str, Union[str, int]]] = []\n",
        "    for (\n",
        "        res\n",
        "    ) in target_chain.get_polymer():  # polymer residues only (skips ligands/solvent)\n",
        "        raw3 = res.name.upper()\n",
        "        can3 = canonicalize_resname(raw3)\n",
        "\n",
        "        # Decide whether to keep\n",
        "        is_canonical = can3 in STANDARD_AA3\n",
        "        if not is_canonical and not include_noncanonical:\n",
        "            # skip unknown/unmapped non-amino-acid residues (e.g., caps ACE/NME, ligands)\n",
        "            continue\n",
        "\n",
        "        one = ONE_LETTER[can3]\n",
        "        seqnum = res.seqid.num\n",
        "        icode = (\n",
        "            res.seqid.icode if res.seqid.icode != \"\\x00\" else \"\"\n",
        "        )  # handle insertion codes, empty if none\n",
        "\n",
        "        out.append(\n",
        "            {\n",
        "                \"resname_raw\": raw3,  # as in file\n",
        "                \"resname\": can3,  # canonicalized 3-letter\n",
        "                \"one_letter\": one,\n",
        "                \"seqnum\": seqnum,\n",
        "                \"icode\": icode,\n",
        "                \"chain\": chain_id,\n",
        "            }\n",
        "        )\n",
        "\n",
        "    return out\n",
        "\n",
        "def convert_reslist_to_seq(res_list: List[Dict[str, Union[str, int]]], pad_negative: bool = False) -> str:\n",
        "    \"\"\"\n",
        "    Convert a list of residue information dictionaries to a linear amino acid sequence.\n",
        "\n",
        "    Builds a sequence from residue number 1 to the maximum residue number,\n",
        "    handling gaps, insertion codes, and optionally negative residue numbers.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    res_list : List[Dict[str, Union[str, int]]]\n",
        "        List of residue dictionaries, each containing:\n",
        "        - resname: 3-letter residue code\n",
        "        - one_letter: 1-letter residue code\n",
        "        - seqnum: Residue sequence number\n",
        "        - icode: Insertion code (if any)\n",
        "    pad_negative : bool, optional\n",
        "        If True, shifts negative residue numbers to positive by adding an offset,\n",
        "        default False\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    str\n",
        "        Linear amino acid sequence where:\n",
        "        - '*' represents missing residues\n",
        "        - For residues with same sequence number (insertions),\n",
        "          prefers the one without insertion code\n",
        "\n",
        "    Notes\n",
        "    -----\n",
        "    The function handles several edge cases:\n",
        "    1. Gaps in numbering are filled with '*'\n",
        "    2. Multiple residues at same position (insertions) are resolved by\n",
        "       preferring the one without insertion code\n",
        "    3. Negative sequence numbers can be shifted positive with pad_negative=True\n",
        "    \"\"\"\n",
        "    if not res_list:\n",
        "        return \"\"\n",
        "\n",
        "    # Determine length (max seqnum). Ignore non-positive seqnums for the linear sequence.\n",
        "    if pad_negative:\n",
        "        min_seqnum = min(\n",
        "            (r[\"seqnum\"] for r in res_list if isinstance(r.get(\"seqnum\"), int)),\n",
        "            default=0,\n",
        "        )\n",
        "        if min_seqnum <= 0:\n",
        "            pad_val = 1 - min_seqnum\n",
        "\n",
        "            res_list = [\n",
        "                {\n",
        "                    **r,\n",
        "                    \"seqnum\": r[\"seqnum\"] + pad_val,\n",
        "                }\n",
        "                for r in res_list\n",
        "            ]\n",
        "\n",
        "    max_seqnum = max(\n",
        "        (r[\"seqnum\"] for r in res_list if isinstance(r.get(\"seqnum\"), int)), default=0\n",
        "    )\n",
        "    if max_seqnum <= 0:\n",
        "        return \"\"\n",
        "\n",
        "    seq = [\"*\"] * max_seqnum\n",
        "    chosen_icode = {}  # seqnum -> icode used\n",
        "\n",
        "    for r in res_list:\n",
        "        seqnum = r.get(\"seqnum\")\n",
        "        if not isinstance(seqnum, int) or seqnum <= 0 or seqnum > max_seqnum:\n",
        "            continue\n",
        "\n",
        "        # Determine one-letter (use provided if present, else map from canonical 3-letter)\n",
        "        one = (r.get(\"one_letter\") or \"\").strip()\n",
        "        if not one:\n",
        "            can3 = (r.get(\"resname\") or \"\").upper()\n",
        "            one = ONE_LETTER[can3]\n",
        "\n",
        "        icode = r.get(\"icode\", \"\") or \"\"\n",
        "        cur = seq[seqnum - 1]\n",
        "\n",
        "        if cur == \"*\":\n",
        "            # nothing placed yet at this position\n",
        "            seq[seqnum - 1] = one\n",
        "            chosen_icode[seqnum] = icode\n",
        "        else:\n",
        "            # Something is already there; resolve by preferring empty icode\n",
        "            prev_icode = chosen_icode.get(seqnum, \"\")\n",
        "            if prev_icode and not icode:\n",
        "                # replace a letter that came from an inserted residue with the main one\n",
        "                seq[seqnum - 1] = one\n",
        "                chosen_icode[seqnum] = icode\n",
        "            # else: keep the first seen\n",
        "\n",
        "    return \"\".join(seq)\n",
        "\n",
        "def get_chain_seq(pdb: str, chain: str) -> str:\n",
        "    \"\"\"\n",
        "    Extract the complete amino acid sequence from a specific chain in a PDB file.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    pdb : str\n",
        "        Path to the PDB file\n",
        "    chain : str\n",
        "        Chain identifier (e.g., 'A')\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    str\n",
        "        Complete amino acid sequence including gaps ('*' for missing residues)\n",
        "        and padded negative residue numbers if present\n",
        "    \"\"\"\n",
        "    return convert_reslist_to_seq(list_chain_residues(pdb, chain), pad_negative=True)\n",
        "\n",
        "def get_chain_seq_for_scoring(pdb: str, chain: str) -> str:\n",
        "    \"\"\"\n",
        "    Get the scorable sequence from a chain by removing unresolved residues.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    pdb : str\n",
        "        Path to the PDB file\n",
        "    chain : str\n",
        "        Chain identifier (e.g., 'A')\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    str\n",
        "        Amino acid sequence containing only resolved residues\n",
        "        (unresolved '*' positions removed)\n",
        "    \"\"\"\n",
        "    return get_chain_seq(pdb, chain).replace(\"*\", \"\")\n",
        "\n",
        "def list_chains(\n",
        "    path: Union[str, Path],\n",
        "    *,\n",
        "    by_model: bool = False,\n",
        "    unique: bool = True,\n",
        "    min_length: int = 5,\n",
        ") -> List[Union[str, Tuple[str, str]]]:\n",
        "    \"\"\"\n",
        "    List chain IDs in a structure file using Gemmi, optionally filtering by minimum\n",
        "    number of amino-acid residues (strictly greater than `min_length`).\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    path : str | Path\n",
        "        Input structure file (.pdb/.cif, gz accepted by gemmi).\n",
        "    by_model : bool\n",
        "        If True, return (model_name, chain_id) pairs (duplicates allowed).\n",
        "        If False, return chain_id strings (unique by default).\n",
        "    unique : bool\n",
        "        When by_model=False, return only unique chain IDs across models.\n",
        "    min_length : int\n",
        "        Only include chains with > min_length amino-acid residues.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    list[str] | list[tuple[str, str]]\n",
        "        Chain IDs (or (model, chain) pairs if by_model=True).\n",
        "    \"\"\"\n",
        "    st = gemmi.read_structure(str(path))\n",
        "\n",
        "    if by_model:\n",
        "        out: List[Tuple[str, str]] = []\n",
        "        for model in st:\n",
        "            for chain in model:\n",
        "                if len(list_chain_residues(path, chain.name)) > min_length:\n",
        "                    out.append((model.name, chain.name))\n",
        "        return out\n",
        "\n",
        "    seen = set()\n",
        "    out: List[str] = []\n",
        "    for model in st:\n",
        "        for chain in model:\n",
        "            if len(list_chain_residues(path, chain.name)) >= min_length:\n",
        "                cid = chain.name\n",
        "                if not unique or cid not in seen:\n",
        "                    out.append(cid)\n",
        "                    seen.add(cid)\n",
        "    return out\n",
        "\n",
        "##############################################\n",
        "# Visualization functions\n",
        "##############################################\n",
        "def view_chains_row(\n",
        "    path: Union[str, Path],\n",
        "    *,\n",
        "    chains: Optional[Sequence[str]] = None,  # e.g., [\"A\",\"B\"]; None -> auto-detect\n",
        "    style: str = \"cartoon\",                  # \"cartoon\" | \"stick\" | \"line\" | \"sphere\"\n",
        "    background: str = \"white\",\n",
        "    spin: bool = False,\n",
        "    width_per: int = 350,\n",
        "    height: int = 275,\n",
        "):\n",
        "    \"\"\"\n",
        "    Visualize one local structure file as multiple side-by-side viewers—one per chain.\n",
        "\n",
        "    - Only supports local, uncompressed .pdb/.cif/.mmcif\n",
        "    - Hides heteroatoms and waters automatically\n",
        "    - If `chains` is None, tries to auto-detect chain IDs\n",
        "      (uses gemmi if installed; for .pdb falls back to a lightweight parser)\n",
        "\n",
        "    Returns the rendered py3Dmol viewer.\n",
        "    \"\"\"\n",
        "    p = Path(path)\n",
        "    if p.suffix.lower() == \".gz\":\n",
        "        raise ValueError(\"Compressed files are not supported. Provide an uncompressed .pdb/.cif/.mmcif.\")\n",
        "    if p.suffix.lower() not in (\".pdb\", \".cif\", \".mmcif\"):\n",
        "        raise ValueError(f\"Unsupported file type: {p.name}\")\n",
        "    if not p.exists():\n",
        "        raise FileNotFoundError(p)\n",
        "\n",
        "    fmt = \"pdb\" if p.suffix.lower() == \".pdb\" else \"cif\"\n",
        "    data = p.read_text()\n",
        "\n",
        "    # Auto-detect chains if not provided\n",
        "    if chains is None:\n",
        "        chains = list_chains(path)\n",
        "\n",
        "    # Normalize chains to strings\n",
        "    chains = [str(c) for c in chains]\n",
        "    if len(chains) == 0:\n",
        "        raise ValueError(\"No chains found to display (after auto-detect). Provide `chains=[...]` explicitly?\")\n",
        "\n",
        "    n = len(chains)\n",
        "    total_width = max(300, int(width_per) * n)\n",
        "\n",
        "    v = py3Dmol.view(width=total_width, height=height, viewergrid=(1, n))\n",
        "\n",
        "    style_map = {\n",
        "        \"cartoon\": {\"cartoon\": {\"colorscheme\": \"chain\"}},\n",
        "        \"stick\":   {\"stick\": {\"radius\": 0.2}},\n",
        "        \"line\":    {\"line\": {}},\n",
        "        \"sphere\":  {\"sphere\": {\"scale\": 0.25}},\n",
        "    }\n",
        "    style_cfg = style_map.get(style.lower(), style_map[\"cartoon\"])\n",
        "\n",
        "    for i, cid in enumerate(chains):\n",
        "        v.addModel(data, fmt, viewer=(0, i))\n",
        "        v.setBackgroundColor(background, viewer=(0, i))\n",
        "\n",
        "        # Polymer only (exclude hetero) and exclude water\n",
        "        sel = {\"hetflag\": False, \"not\": {\"resn\": [\"HOH\", \"WAT\"]}, \"chain\": str(cid)}\n",
        "\n",
        "        v.setStyle(sel, style_cfg, viewer=(0, i))\n",
        "        v.zoomTo(sel, viewer=(0, i))\n",
        "        if spin:\n",
        "            v.spin(True, viewer=(0, i))\n",
        "\n",
        "    return v.show()\n",
        "\n",
        "def generate_html_headers(content: List[tuple], width: int, section_gap = 10, bottom_pad = 4, border = 2, border_radius = 8, padding = 12):\n",
        "    n_sections = len(content)\n",
        "    section_widths = [width - section_gap - padding*2 - border*2] * n_sections\n",
        "    section_widths[0] += section_gap/2\n",
        "    section_widths[-1] += section_gap/2\n",
        "\n",
        "    output = f\"\"\"<div style=\"display:flex;flex-wrap:wrap;gap:{section_gap}px;justify-content:left;padding-bottom:{bottom_pad}px\">\"\"\"\n",
        "    for i in range(n_sections):\n",
        "        sec_width = section_widths[i]\n",
        "        new_section = f\"\"\"<section style=\"width:{sec_width}px;text-align:center;border:{border}px solid #ddd;border-radius:{border_radius}px;padding:{padding}px\"><h3 style=\"margin:0 0 6px\">{content[i][0]}</h3><div>{content[i][1]}</div></section>\"\"\"\n",
        "        output += new_section\n",
        "    output += \"\"\"</div>\"\"\"\n",
        "\n",
        "    return HTML(output)\n",
        "\n",
        "##############################################\n",
        "# Sequence Alignment and Mapping Functions\n",
        "##############################################\n",
        "def setup_aligner():\n",
        "    \"\"\"\n",
        "    Use pairwise aligner from Bio.Align, with default alignment settings\n",
        "    - for aligning PDB sequences\n",
        "    \"\"\"\n",
        "    aligner = PairwiseAligner()\n",
        "    aligner.mode = \"global\"\n",
        "    aligner.open_gap_score = -10\n",
        "    aligner.extend_gap_score = -0.5\n",
        "    aligner.match_score = 2\n",
        "    aligner.mismatch_score = -0.5\n",
        "    return aligner\n",
        "\n",
        "def alignment_to_mapping(alignment, allow_mismatches=False):\n",
        "    \"\"\"\n",
        "    Convert an alignment object to a dictionary that maps\n",
        "    sequence A index -> sequence B index\n",
        "\n",
        "    - allow_mismatches: whether aligned regions where AA residues\n",
        "    don't match should be mapped\n",
        "    \"\"\"\n",
        "    mapping = {}\n",
        "    current_a_idx = 0\n",
        "    current_b_idx = 0\n",
        "    n_mismatches = 0\n",
        "\n",
        "    # iterate along sequences and match up indices, skipping gaps\n",
        "    for i, (a, b) in enumerate(zip(alignment[0], alignment[1])):\n",
        "        if a != \"-\" and b != \"-\":\n",
        "            if a == b or allow_mismatches:\n",
        "                mapping[current_a_idx] = current_b_idx\n",
        "            if a != b:\n",
        "                n_mismatches += 1\n",
        "            current_a_idx += 1\n",
        "            current_b_idx += 1\n",
        "        elif a != \"-\":\n",
        "            current_a_idx += 1\n",
        "        elif b != \"-\":\n",
        "            current_b_idx += 1\n",
        "\n",
        "    if n_mismatches > 0:\n",
        "        if allow_mismatches:\n",
        "            print(f\"WARNING: {n_mismatches} positions with AA mismatch included in alignment!\")\n",
        "        else:\n",
        "            print(f\"WARNING: {n_mismatches} positions with AA mismatch not included in alignment!\")\n",
        "\n",
        "    return mapping\n",
        "\n",
        "\n",
        "def mapping_to_sequence(scaffold_seq, target_seq, mapping, fill=True, fill_char=\"A\"):\n",
        "    \"\"\"Use mapping dictionary to convert an input sequence to a sequence\n",
        "    that can be scored on a target structure\n",
        "    - scaffold seq: consensus sequence\n",
        "    - target seq: sequence matching to a pdb\n",
        "    - mapping: generated dict from alignment that maps scaffold seq indices\n",
        "    to target seq indices\n",
        "    \"\"\"\n",
        "\n",
        "    # create temp seq and start filling using scaffold seq and mapping dict\n",
        "    seq = [\"-\"] * len(target_seq) #CHECK: can mapping result in values > len(target_seq)?\n",
        "    for i, c in enumerate(scaffold_seq):\n",
        "        if i in mapping:\n",
        "            seq[mapping[i]] = c\n",
        "\n",
        "    # fill any missing values with target seq\n",
        "    for i in range(len(seq)):\n",
        "        if seq[i] == \"-\":\n",
        "            seq[i] = target_seq[i]\n",
        "\n",
        "    # in edge case where some positions are unfilled, either fill or throw error\n",
        "    if fill:\n",
        "        filled = 0\n",
        "        for i, val in enumerate(seq):\n",
        "            if val not in AA_ALPHABET:\n",
        "                seq[i] = fill_char\n",
        "                filled += 1\n",
        "        if filled > 0:\n",
        "            print(f\"WARNING: Filled in {filled} missing AAs with {fill_char}\")\n",
        "    else:\n",
        "        for i, val in enumerate(seq):\n",
        "            if val not in AA_ALPHABET:\n",
        "                raise ValueError(f\"Invalid character ({val}) in position {i}, {seq}\")\n",
        "\n",
        "    return \"\".join(seq)\n",
        "\n",
        "\n",
        "def make_consensus_sequence(sequences):\n",
        "    \"\"\"takes input sequences for many pdbs and generates a consensus sequence,\n",
        "    preferring by default earlier sequences in the input list if there are\n",
        "    mismatches\n",
        "    if any noncanonical AAs are encountered, adds * character\n",
        "    \"\"\"\n",
        "    aligner = setup_aligner()\n",
        "    template_seq = sequences[0]\n",
        "    for seq in sequences[1:]:\n",
        "        alignment = aligner.align(template_seq, seq)[0]\n",
        "        seq1, seq2 = alignment[0], alignment[1]\n",
        "        template_seq = \"\"\n",
        "        for a, b in zip(seq1, seq2):\n",
        "            if a in AA_ALPHABET:\n",
        "                template_seq += a\n",
        "            elif b in AA_ALPHABET:\n",
        "                template_seq += b\n",
        "            else:\n",
        "                template_seq += \"*\"\n",
        "    return template_seq\n",
        "\n",
        "##############################################\n",
        "# CB run counter\n",
        "##############################################\n",
        "import requests\n",
        "\n",
        "counter_app_url = 'https://script.google.com/macros/s/AKfycbzyn9yDouBjXc50N8TRR0f6cHwsRhnZUQd_vMnJRIw12-z1xifSXAh2Atxlkm9s_S46/exec'\n",
        "\n",
        "def increment_cb_run_count():\n",
        "    url =  counter_app_url#web app that increments a counter\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        print(\"Run count incremented successfully.\")\n",
        "    else:\n",
        "        print(\"Failed to increment count.\")\n",
        "\n",
        "##############################################\n",
        "# Scoring Functions\n",
        "##############################################\n",
        "def norm_scale(inp):\n",
        "    '''helper to standardize scores'''\n",
        "    mean_val = np.mean(inp)\n",
        "    std_val = np.std(inp)\n",
        "    if std_val == 0:\n",
        "        return np.zeros_like(inp)\n",
        "    return (inp - mean_val) / std_val\n",
        "\n",
        "def add_scaled_outputs(df:pd.DataFrame, model: str):\n",
        "    '''helper function to scale outputs and combine'''\n",
        "    df[f\"{model}_state1_scaled\"] = norm_scale(df[f\"{model}_state1\"])\n",
        "    df[f\"{model}_state2_scaled\"] = norm_scale(df[f\"{model}_state2\"])\n",
        "    df[f\"{model}_state1_bias\"] = df[f\"{model}_state1_scaled\"] - df[f\"{model}_state2_scaled\"]\n",
        "    df[f\"{model}_state2_bias\"] = df[f\"{model}_state2_scaled\"] - df[f\"{model}_state1_scaled\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "fTeMV9C0-gwj"
      },
      "outputs": [],
      "source": [
        "#@title Input Structure Configuration\n",
        "# This cell handles the input of protein structures for conformational biasing:\n",
        "# - Accepts PDB codes, AlphaFold DB accessions, or custom structure uploads\n",
        "# - Supports both PDB and mmCIF formats\n",
        "# - Validates and processes input structures for both conformational states\n",
        "\n",
        "#@markdown ### Structure Selection for State 1 and State 2\n",
        "#@markdown Please see above for best structure guidelines. In brief, structures should be for the same protein, with good coverage and similar resolution, with no/minimal point mutations. Custom upload structures can be PDB or mmCIF.\n",
        "\n",
        "#@markdown **Acceptable inputs:** PDB ID (4-letter code), AlphaFold DB accession code, custom file upload (leave text entry box empty)\n",
        "\n",
        "state1_pdb = \"3A7R\" # @param {\"type\":\"string\",\"placeholder\":\"PDB code or AFDB accession (leave blank to upload custom structure)\"}\n",
        "state2_pdb = \"1X2G\" #@param {type:\"string\",\"placeholder\":\"PDB code or AFDB accession (leave blank to upload custom structure)\"}\n",
        "#@markdown ---\n",
        "\n",
        "state1_path = get_pdb(state1_pdb, upload_message = 'Upload State 1 Structure', upload_out_path = 'state1.pdb')\n",
        "state2_path = get_pdb(state2_pdb, upload_message = 'Upload State 2 Structure', upload_out_path = 'state2.pdb')\n",
        "\n",
        "#@markdown ### Structure Viewer - Verify Chains\n",
        "#@markdown Please verify that your structures look correct, and which chain per structure is the proper one to score on. Rerun this cell to update the structures.\n",
        "\n",
        "# - Displays 3D visualization of each chain\n",
        "# - Shows chain statistics (length, resolved residues)\n",
        "# - Helps validate structure selection and chain identification\n",
        "# - Enables comparison between conformational states\n",
        "\n",
        "state1_chains = list_chains(state1_path)\n",
        "state2_chains = list_chains(state2_path)\n",
        "\n",
        "max_cols = 5\n",
        "vis_width = 350\n",
        "\n",
        "#splits chains into smaller lists so not too many are visualized per row\n",
        "state1_chains_batch = [state1_chains[i:i+max_cols] for i in range(0, len(state1_chains), max_cols)]\n",
        "state2_chains_batch = [state2_chains[i:i+max_cols] for i in range(0, len(state2_chains), max_cols)]\n",
        "\n",
        "#visualize for state 1\n",
        "display(HTML(\"\"\"<h3>State 1 Structure Visualization</h3>\"\"\"))\n",
        "for vis_batch in state1_chains_batch:\n",
        "    chain_seqs = {chain: get_chain_seq(state1_path, chain) for chain in vis_batch}\n",
        "    chain_seqs_scoring = {chain: get_chain_seq_for_scoring(state1_path, chain) for chain in vis_batch}\n",
        "    chain_info = [(f\"{state1_path}, chain {chain}\", f\"{len(chain_seqs_scoring[chain])} AAs resolved / {len(chain_seqs[chain])} AAs total\") for chain in chain_seqs]\n",
        "    display(generate_html_headers(chain_info, width = vis_width))\n",
        "\n",
        "    view_chains_row(state1_path, chains = vis_batch, width_per=vis_width)\n",
        "\n",
        "print(\"\\n\")\n",
        "#visualize for state 2\n",
        "display(HTML(\"\"\"<h3>State 2 Structure Visualization</h3>\"\"\"))\n",
        "for vis_batch in state2_chains_batch:\n",
        "    chain_seqs = {chain: get_chain_seq(state2_path, chain) for chain in vis_batch}\n",
        "    chain_seqs_scoring = {chain: get_chain_seq_for_scoring(state2_path, chain) for chain in vis_batch}\n",
        "    chain_info = [(f\"{state2_path}, chain {chain}\", f\"{len(chain_seqs_scoring[chain])} AAs resolved / {len(chain_seqs[chain])} AAs total\") for chain in chain_seqs]\n",
        "    display(generate_html_headers(chain_info, width = vis_width))\n",
        "\n",
        "    view_chains_row(state2_path, chains = vis_batch, width_per = vis_width)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "P-wTRKihSqHf"
      },
      "outputs": [],
      "source": [
        "#@title Chain Configuration, Sequence Alignment, and Mutant Generation\n",
        "#@markdown Select chains based on structures as shown above.\n",
        "#@markdown This cell will extract sequences from those chains, and generate a consensus sequence that can be aligned to both structures.\n",
        "#@markdown Please check alignment results and visualization to ensure good results.\n",
        "\n",
        "\n",
        "#@markdown Consensus sequence will then be used to generate all single mutants for positions that are resolved in both structures.\n",
        "\n",
        "#@markdown ### Specify State 1 Chain ID\n",
        "state1_chain = \"A\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown ### Specify State 2 Chain ID\n",
        "state2_chain = \"A\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown ### (Optional) Reference Protein Sequence\n",
        "#@markdown Upload a reference sequence to use as a template for sequence alignment.\n",
        "#@markdown Reference sequence should be a superset of the sequences in the uploaded structures.\n",
        "sequence = \"\" #@param {type:\"string\"}\n",
        "\n",
        "if state1_chain not in state1_chains:\n",
        "    raise ValueError(f\"Chain {state1_chain} not found in {state1_path}. Available chains are {state1_chains}.\")\n",
        "if state2_chain not in state2_chains:\n",
        "    raise ValueError(f\"Chain {state2_chain} not found in {state2_path}. Available chains are {state2_chains}.\")\n",
        "\n",
        "pdbs = {\n",
        "    \"state1\": state1_path,\n",
        "    \"state2\": state2_path,\n",
        "}\n",
        "\n",
        "chains = {\n",
        "    'state1': state1_chain,\n",
        "    'state2': state2_chain,\n",
        "}\n",
        "\n",
        "# - Aligns sequences from both conformational states\n",
        "# - Creates consensus sequence\n",
        "# - Identifies mutable positions present in both structures\n",
        "# - Generates all possible single-point mutations\n",
        "# - Provides detailed statistics and visualization of alignments\n",
        "# - Validates sequence coverage and alignment quality\n",
        "\n",
        "#get pdb sequences and align them for input chains\n",
        "seqs = {pdb: get_chain_seq(pdbs[pdb], chains[pdb]) for pdb in pdbs}\n",
        "scoring_seqs = {pdb: get_chain_seq_for_scoring(pdbs[pdb], chains[pdb]) for pdb in pdbs}\n",
        "\n",
        "if len(sequence) == 0:\n",
        "    con_seq = make_consensus_sequence(list(seqs.values()))\n",
        "else:\n",
        "    con_seq = make_consensus_sequence([sequence] + list(seqs.values()))\n",
        "\n",
        "aligner = setup_aligner()\n",
        "alignments = {pdb: aligner.align(con_seq, seq)[0] for pdb, seq in scoring_seqs.items()}\n",
        "\n",
        "mappings = {\n",
        "    pdb: alignment_to_mapping(alignment, allow_mismatches = False) for pdb, alignment in alignments.items()\n",
        "}\n",
        "\n",
        "def print_sequence_with_index(s, n):\n",
        "    \"\"\"\n",
        "    Splits the input string s into chunks of length n and prints each chunk\n",
        "    annotated with its starting index (1-based).\n",
        "    \"\"\"\n",
        "    # Split string into chunks\n",
        "    if len(s) <= n:\n",
        "        chunks = [s]\n",
        "    else:\n",
        "        chunks = [s[i : i + n] for i in range(0, len(s), n)]\n",
        "    max_anno_len = len(str((len(chunks) - 1) * n + 1))\n",
        "    for i, chunk in enumerate(chunks):\n",
        "        anno_str = str(i * n + 1)\n",
        "        if len(anno_str) < max_anno_len:\n",
        "            anno_str += \" \" * (max_anno_len - len(anno_str))\n",
        "        print(f\"{anno_str} {chunk}\")\n",
        "\n",
        "# Print consensus sequence with index\n",
        "display(HTML(\"\"\"\n",
        "<h3>Consensus Sequence Analysis</h3>\n",
        "<p style=\"color: #666; margin-bottom: 10px;\">The consensus sequence represents the common sequence derived from both conformational states.</p>\n",
        "\"\"\"))\n",
        "print(f\"\\033[1mConsensus Sequence\\033[0m (Length: {len(con_seq)})\")\n",
        "print_sequence_with_index(con_seq, 50)\n",
        "print()\n",
        "\n",
        "# Display sequence statistics\n",
        "display(HTML(\"\"\"\n",
        "<h3>Structure Coverage Statistics</h3>\n",
        "<p style=\"color: #666; margin-bottom: 10px;\">Analysis of amino acid positions in each conformational state structure.</p>\n",
        "\"\"\"))\n",
        "print(\"\\033[1mPosition Statistics:\\033[0m\")\n",
        "print(f\"• State 1: {len(scoring_seqs['state1'])}/{len(seqs['state1'])} AAs resolved ({(len(scoring_seqs['state1'])/len(seqs['state1'])*100):.1f}%)\")\n",
        "print(f\"• State 2: {len(scoring_seqs['state2'])}/{len(seqs['state2'])} AAs resolved ({(len(scoring_seqs['state2'])/len(seqs['state2'])*100):.1f}%)\")\n",
        "print()\n",
        "\n",
        "# Visualize sequence alignments\n",
        "display(HTML(\"\"\"\n",
        "<h3>Sequence Alignment Analysis</h3>\n",
        "<p style=\"color: #666; margin-bottom: 10px;\">Validation of structure alignments. Check for potential issues with structure selection or coverage.</p>\n",
        "\"\"\"))\n",
        "print(\"\\033[1mAlignment Scores:\\033[0m\")\n",
        "print(f\"• State 1 → Consensus: {alignments['state1'].score:.1f}\")\n",
        "print(f\"• State 2 → Consensus: {alignments['state2'].score:.1f}\")\n",
        "print()\n",
        "print(\"\\033[1;33mNote:\\033[0m Poor alignments or negative scores may indicate incorrect structures or poor coverage.\")\n",
        "print(\"Thick line represents aligned regions.\")\n",
        "\n",
        "# Find positions present in all structures\n",
        "positions_present = []\n",
        "for i in range(len(con_seq)):\n",
        "    if (i in mappings['state1']) and (i in mappings['state2']) and con_seq[i] in AA_ALPHABET:\n",
        "        positions_present.append(i)\n",
        "\n",
        "con_track = [i for i in range(len(con_seq)) if con_seq[i] in AA_ALPHABET]\n",
        "state1_track = [i for i in con_track if i in mappings[\"state1\"]]\n",
        "state2_track = [i for i in con_track if i in mappings[\"state2\"]]\n",
        "\n",
        "plt.figure(figsize=(10, 1))\n",
        "plt.plot(con_track, [1.25] * len(con_track), marker=\".\")\n",
        "plt.plot(state1_track, [1] * len(state1_track), marker = \".\")\n",
        "plt.plot(state2_track, [0.75] * len(state2_track), marker = \".\")\n",
        "plt.ylim(0.5, 1.5)\n",
        "test = plt.xticks()[1][0]\n",
        "\n",
        "def parse_text(s):\n",
        "    \"\"\"helper to fix xtick labels\"\"\"\n",
        "    if s[0].isnumeric():\n",
        "        return int(s)\n",
        "    else:\n",
        "        return -1*int(s[1:])\n",
        "\n",
        "plt.xticks(plt.xticks()[0], [parse_text(i.get_text()) + 1 for i in plt.xticks()[1]])\n",
        "plt.xlim(-10, len(con_seq) + 10)\n",
        "plt.yticks([0.75, 1, 1.25], ['State 2 Structure', 'State 1 Structure', 'Consensus Sequence'])\n",
        "plt.xlabel(\"Residue Position\")\n",
        "plt.title('Structure/Sequence Alignments')\n",
        "plt.show()\n",
        "\n",
        "# Generate mutants and create dataframe\n",
        "mutants = []\n",
        "mutant_sequences = []\n",
        "mutant_ids = []  # for simplifying workflow with uploaded sequences\n",
        "for i in positions_present:\n",
        "    aa = con_seq[i]\n",
        "    for aa_new in AA_ALPHABET:\n",
        "        if aa_new != aa:\n",
        "            mutant_sequences.append(con_seq[:i] + aa_new + con_seq[i + 1 :])\n",
        "            mutants.append(f\"{aa}{i+1}{aa_new}\")\n",
        "            mutant_ids.append(f\"SM-{aa}{i+1}{aa_new}\")\n",
        "\n",
        "display(HTML(\"\"\"\n",
        "<h3>Mutant Statistics</h3>\n",
        "<p style=\"color: #666; margin-bottom: 10px;\">Information about how many single mutant sequences were generated vs. total possible.</p>\n",
        "\"\"\"))\n",
        "# Print mutation statistics\n",
        "print(\"\\033[1mMutation Statistics:\\033[0m\")\n",
        "print(f\"• Mutable positions: {len(positions_present)}/{len(con_seq)} AA positions ({(len(positions_present)/len(con_seq)*100):.1f}%)\")\n",
        "print(f\"• Generated mutations: {len(mutants)}/{len(con_seq) * 19} possible point mutations ({(len(mutants)/(len(con_seq) * 19)*100):.1f}%)\")\n",
        "print()\n",
        "\n",
        "output = pd.DataFrame({'id': mutant_ids, 'mutant': mutants, 'sequence': mutant_sequences})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "co4oeVuFiuHW"
      },
      "outputs": [],
      "source": [
        "#@title Set CB run parameters and upload custom sequences\n",
        "#@markdown ### Name CB Job\n",
        "#@markdown Name will be used for file naming.\n",
        "\n",
        "jobname = \"CB-LplA\" #@param {type:\"string\"}\n",
        "#@markdown ---\n",
        "#@markdown ### (Optional) Upload custom sequences for scoring\n",
        "#@markdown If \"yes\", you will be prompted for file upload.\n",
        "#@markdown Uploaded file should be in fasta format. Sequences should have the same length as the consensus sequence.\n",
        "#@markdown Any sequences that are not accepted will raise an error and reset sequences to be scored to the previous state.\n",
        "upload_custom_sequences = \"no\" # @param [\"no\",\"yes\"]\n",
        "#@markdown ---\n",
        "\n",
        "from Bio import SeqIO\n",
        "\n",
        "#uploader for custom sequences\n",
        "def get_sequence_fasta(fasta_dir = './fasta_uploads/'):\n",
        "    display(HTML(f\"<h3>Upload Custom Fasta File</h3>\"))\n",
        "    upload_dict = files.upload()\n",
        "\n",
        "    os.makedirs(fasta_dir, exist_ok = True)\n",
        "    out_path = os.path.join(fasta_dir, rand_id() + '.fa')\n",
        "    try:\n",
        "        first_upload_key = [\n",
        "            k\n",
        "            for k in upload_dict\n",
        "            if (k.endswith(\".fa\") or k.endswith(\".fasta\"))\n",
        "        ][0]\n",
        "        with open(out_path, \"wb\") as out:\n",
        "            out.write(upload_dict[first_upload_key])\n",
        "\n",
        "        for file in upload_dict:\n",
        "            if os.path.exists(file):\n",
        "                os.remove(file)\n",
        "        return out_path\n",
        "    except:\n",
        "        for file in upload_dict:\n",
        "            if os.path.exists(file):\n",
        "                os.remove(file)\n",
        "        raise FileNotFoundError(\"No fasta file uploaded! (extensions .fa and .fasta accepted)\")\n",
        "\n",
        "if upload_custom_sequences == \"yes\":\n",
        "    new_output_rows = []\n",
        "    fasta_fp = get_sequence_fasta()\n",
        "    added_counter = 0\n",
        "    invalid_inputs = []\n",
        "\n",
        "    #consensus sequence may have some blank positions that are no scored\n",
        "    con_seq_aa_positions = [i for i in range(len(con_seq)) if con_seq[i] in AA_ALPHABET]\n",
        "    for record in SeqIO.parse(fasta_fp, \"fasta\"):\n",
        "        id = record.id\n",
        "        seq = record.seq\n",
        "\n",
        "        #check for proper formatting\n",
        "        if len(seq) != len(con_seq):\n",
        "            print(f'Skipped {id} due to length mismatch - {len(seq)} vs. {len(con_seq)} (consensus).')\n",
        "            invalid_inputs.append(id)\n",
        "        elif not all(aas in AA_ALPHABET for aas in [seq[i] for i in con_seq_aa_positions]):\n",
        "            print(f'Skipped {id} due to invalid characters in input sequence.')\n",
        "            invalid_inputs.append(id)\n",
        "        elif seq in set(output['sequence']):\n",
        "            print(f'Skipped {id} due to sequence being already present sequences to be scored.')\n",
        "        else:\n",
        "            added_counter += 1\n",
        "\n",
        "            id = id.replace(\" \", \"_\")\n",
        "            id = id.replace(\",\", \"\")\n",
        "            if len(id) == 0:\n",
        "                id = str(added_counter)\n",
        "            cu_id = 'CU-' + id\n",
        "\n",
        "            new_output_rows.append({'id': cu_id, 'mutant': None, 'sequence': seq})\n",
        "            print(f'Listed {id} as {cu_id}.')\n",
        "\n",
        "    if len(invalid_inputs) > 0:\n",
        "        #do this to make sure user scores all sequences they want to\n",
        "        print(f\"\\nReset sequences to previous state due to {len(invalid_inputs)} invalid input sequences in upload. Please fix and reupload. \")\n",
        "    else:\n",
        "        new_rows_df = pd.DataFrame(new_output_rows)\n",
        "        output = pd.concat([output, new_rows_df], ignore_index = True)\n",
        "        print(f\"\\nSuccessfully added {added_counter} new sequences to CB run.\")\n",
        "\n",
        "#throw an error if not all sequences added are added successfully - and reset output\n",
        "#@markdown ### CB models to run\n",
        "#@markdown Default: ProteinMPNN only. ESM-IF1 is slow comparatively, and ThermoMPNN will only score single mutants (no custom sequences).\n",
        "run_proteinmpnn = \"yes\" # @param [\"no\",\"yes\"]\n",
        "run_frame2seq = \"no\" # @param [\"no\",\"yes\"]\n",
        "run_thermompnn = \"no\" # @param [\"no\",\"yes\"]\n",
        "run_esmif1 = \"no\" # @param [\"no\",\"yes\"]\n",
        "#@markdown ---\n",
        "#@markdown #### **<font color='red'>WARNING: Re-running this code block will reset any previously generated model outputs!</font>**\n",
        "\n",
        "model_outputs = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5AZJa_bfc_SL",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Run CB Models\n",
        "\n",
        "#@markdown Please ensure that you are running on a GPU-enabled runtime.\n",
        "#@markdown For a protein of size ~38kDa/340 amino acids, model evaluation time for various GPUs is approximately:\n",
        "#@markdown - **ProteinMPNN**: 3 min *(T4)*, 1 min *(L40)*, 1 min *(A100)*\n",
        "#@markdown - **Frame2Seq**: &lt;1min *(T4, L40, A100)*\n",
        "#@markdown - **ThermoMPNN**: &lt;1min *(T4, L40, A100)*\n",
        "#@markdown - **ESM-IF1**: 1 hour *(T4)*, 30 min *(L40)*, 20 min *(A100)*\n",
        "\n",
        "#@markdown If your execution is taking significantly longer, please open an issue with detailed description of your inputs on our github repository.\n",
        "\n",
        "\n",
        "### PROTEINMPNN\n",
        "if run_proteinmpnn == 'yes':\n",
        "    #simple scoring function\n",
        "    def mpnn_score(seq, model, return_indiv=False):\n",
        "        '''use AR mask and seq to calculate PMPNN log likelihood'''\n",
        "        L = len(seq)\n",
        "        ar_mask = 1 - np.eye(L)\n",
        "        outputs = model.score(seq=seq, ar_mask=ar_mask)\n",
        "        pssm = softmax(outputs[\"logits\"], -1)\n",
        "        probs = np.squeeze(pssm[outputs[\"S\"] == 1])\n",
        "\n",
        "        if return_indiv:\n",
        "            return np.log(probs)\n",
        "        else:\n",
        "            return np.log(probs).sum()\n",
        "\n",
        "    print(f'Loading ProteinMPNN model...')\n",
        "    homooligomer = False  # if structure is a homooligomer\n",
        "    fix_pos = None\n",
        "    inverse = True  # whether to invert the fix pos selection\n",
        "    model_name = \"v_48_020\"  # [\"v_48_002\", \"v_48_010\", \"v_48_020\", \"v_48_030\"]\n",
        "\n",
        "    pmpnn_outputs = output.copy()\n",
        "\n",
        "    if \"mpnn_model\" not in dir():\n",
        "        mpnn_model = mk_mpnn_model(model_name)\n",
        "    print(f'ProteinMPNN model loaded.\\n')\n",
        "\n",
        "    for structure in pdbs:\n",
        "        print(f'Running ProteinMPNN on {structure}...')\n",
        "        output_scores = []\n",
        "\n",
        "        #load structure\n",
        "        mpnn_model.prep_inputs(\n",
        "            pdb_filename=pdbs[structure],\n",
        "            chain=chains[structure],\n",
        "            homooligomer=homooligomer,\n",
        "            fix_pos=fix_pos,\n",
        "            inverse=inverse,\n",
        "            verbose=True,\n",
        "        )\n",
        "\n",
        "        #score wt\n",
        "        wt_seq = mapping_to_sequence(con_seq, scoring_seqs[structure], mappings[structure])\n",
        "        wt_score = mpnn_score(wt_seq, mpnn_model)\n",
        "\n",
        "        #score all mut sequences\n",
        "        for mut_seq in tqdm(pmpnn_outputs['sequence']):\n",
        "            mapped_seq = mapping_to_sequence(mut_seq, scoring_seqs[structure], mappings[structure])\n",
        "            score = mpnn_score(mapped_seq, mpnn_model)\n",
        "            output_scores.append(score - wt_score)\n",
        "\n",
        "        pmpnn_outputs[\"proteinmpnn_\" + structure] = output_scores\n",
        "\n",
        "    #scale variant scores and add bias values\n",
        "    add_scaled_outputs(pmpnn_outputs, \"proteinmpnn\")\n",
        "    model_outputs['proteinmpnn'] = pmpnn_outputs\n",
        "    print('ProteinMPNN evaluation complete.')\n",
        "\n",
        "### FRAME2SEQ\n",
        "if run_frame2seq == 'yes':\n",
        "    try:\n",
        "        import frame2seq\n",
        "    except:\n",
        "        os.system(\"pip -q install git+https://github.com/andrewxue98/Frame2seq.git\")\n",
        "\n",
        "    from tqdm.notebook import tqdm\n",
        "    import torch\n",
        "\n",
        "    from frame2seq.utils import residue_constants\n",
        "    from frame2seq.utils.util import get_neg_pll\n",
        "    from frame2seq.utils.pdb2input import get_inference_inputs\n",
        "\n",
        "    def frame2seq_score(\n",
        "        runner,\n",
        "        pdb_file: str,\n",
        "        chain_id: str,\n",
        "        input_seqs: list[str]\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Calculates the pseudo-log-likelihood (PLL) scores for a list of input sequences\n",
        "        given a structure using a Frame2seq model ensemble.\n",
        "\n",
        "        Args:\n",
        "            runner: Frame2seqRunner.\n",
        "                An initialized Frame2seqRunner object containing the ensemble models.\n",
        "            pdb_file: str\n",
        "                Path to a PDB file containing the desired protein structure.\n",
        "            chain_id: str\n",
        "                Chain identifier (e.g., 'A') corresponding to the chain of interest in the PDB file.\n",
        "            input_seqs: list of str\n",
        "                List of amino acid sequences to be evaluated against the structure. Must be length matched.\n",
        "\n",
        "        Returns:\n",
        "            scores: list of float\n",
        "                List of negative PLL scores, one for each input sequence. Higher (less negative)\n",
        "                values indicate sequences more compatible with the structure.\n",
        "        \"\"\"\n",
        "        # Get structure-based input tensors for inference\n",
        "        seq_mask, backbone_seq_tokenized, X = get_inference_inputs(pdb_file, chain_id)\n",
        "\n",
        "        # Decode backbone sequence from tokenized integer representation\n",
        "        backbone_seq = [residue_constants.ID_TO_AA[int(i)] for i in backbone_seq_tokenized[0]]\n",
        "\n",
        "        # Convert backbone sequence to one-hot encoding using standard AA to ID mapping\n",
        "        backbone_seq_onehot = residue_constants.sequence_to_onehot(\n",
        "            sequence=backbone_seq,\n",
        "            mapping=residue_constants.AA_TO_ID,\n",
        "        )\n",
        "\n",
        "        # Convert one-hot numpy array to torch tensor and move to runner device\n",
        "        backbone_seq_onehot = (\n",
        "            torch.from_numpy(backbone_seq_onehot).float().unsqueeze(0).to(runner.device)\n",
        "        )\n",
        "        # Mask all positions in sequence by setting them to 'X' (unknown amino acid)\n",
        "        backbone_seq_onehot = torch.zeros_like(backbone_seq_onehot)\n",
        "        backbone_seq_onehot[:, :, 20] = 1  # 20 = 'X', mask all positions\n",
        "\n",
        "        scores = []  # list to collect scores for each input sequence\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # Run all three ensemble models to get amino acid probabilities\n",
        "            aaprobs1 = runner.models[0].forward(X, seq_mask, backbone_seq_onehot)\n",
        "            aaprobs2 = runner.models[1].forward(X, seq_mask, backbone_seq_onehot)\n",
        "            aaprobs3 = runner.models[2].forward(X, seq_mask, backbone_seq_onehot)\n",
        "\n",
        "            # Average logits from ensemble models\n",
        "            aaprobs = (aaprobs1 + aaprobs2 + aaprobs3) / 3  # ensemble model predictions\n",
        "\n",
        "            # Apply softmax to obtain amino acid probability distributions\n",
        "            aaprobs = torch.nn.functional.softmax(aaprobs, dim=-1)\n",
        "\n",
        "            # Only keep probabilities at valid sequence mask positions\n",
        "            aaprobs = aaprobs[seq_mask]\n",
        "\n",
        "            # Convert each input sequence to tensor of residue IDs on the runner device\n",
        "            input_seqs = [\n",
        "                torch.tensor([residue_constants.AA_TO_ID[aa] for aa in seq])\n",
        "                .long()\n",
        "                .to(runner.device)\n",
        "                for seq in input_seqs\n",
        "            ]\n",
        "\n",
        "            # For each input sequence, calculate and collect the negative PLL score (log-likelihood under model)\n",
        "            for sample in tqdm(range(len(input_seqs))):\n",
        "                input_seq_i = input_seqs[sample]\n",
        "                _neg_pll, avg_neg_pll = get_neg_pll(aaprobs, input_seq_i)\n",
        "                scores.append(-1 * avg_neg_pll)  # multiply by -1 to return PLL\n",
        "\n",
        "        return scores  # return list of scores, one per input sequence\n",
        "\n",
        "    from frame2seq import Frame2seqRunner\n",
        "    runner = Frame2seqRunner()\n",
        "\n",
        "    print(f'Loading frame2seq model...')\n",
        "    print(f'frame2seq model loaded.\\n')\n",
        "\n",
        "    #align seqs and generate mappings, as before\n",
        "    f2s_seqs = {structure: \"\".join([residue_constants.ID_TO_AA[int(i)] for i in get_inference_inputs(pdbs[structure], chains[structure])[1].squeeze()]) for structure in pdbs}\n",
        "\n",
        "    aligner = setup_aligner()\n",
        "    f2s_alignments = {pdb: aligner.align(con_seq, seq)[0] for pdb, seq in f2s_seqs.items()}\n",
        "\n",
        "    f2s_mappings = {\n",
        "        pdb: alignment_to_mapping(alignment) for pdb, alignment in f2s_alignments.items()\n",
        "    }\n",
        "\n",
        "    f2s_outputs = output.copy()\n",
        "\n",
        "    #evaluate sequences\n",
        "    for structure in pdbs:\n",
        "        print(f'Running frame2seq on {structure}...')\n",
        "        output_seqs = []\n",
        "\n",
        "        wt_seq = mapping_to_sequence(con_seq, f2s_seqs[structure], f2s_mappings[structure])\n",
        "        wt_score = frame2seq_score(runner, pdbs[structure], chains[structure], [wt_seq])[0]\n",
        "\n",
        "        for mut_seq in f2s_outputs['sequence']:\n",
        "            mapped_seq = mapping_to_sequence(\n",
        "                mut_seq, f2s_seqs[structure], f2s_mappings[structure]\n",
        "            )\n",
        "            output_seqs.append(mapped_seq)\n",
        "\n",
        "        outs = frame2seq_score(runner, pdbs[structure], chains[structure], output_seqs)\n",
        "\n",
        "        f2s_outputs[f\"frame2seq_{structure}\"] = [x - wt_score for x in outs]\n",
        "\n",
        "    add_scaled_outputs(f2s_outputs, \"frame2seq\")\n",
        "    model_outputs['frame2seq'] = f2s_outputs\n",
        "    print('frame2seq evaluation complete.')\n",
        "\n",
        "### THERMOMPNN\n",
        "if run_thermompnn == 'yes':\n",
        "    try:\n",
        "        import thermompnn\n",
        "    except:\n",
        "        os.system(\"pip -q install git+https://github.com/andrewxue98/ThermoMPNN.git\")\n",
        "        os.system('wget \"https://raw.githubusercontent.com/andrewxue98/ThermoMPNN/main/weights/thermoMPNN_default.pt\"')\n",
        "        os.system('wget \"https://raw.githubusercontent.com/andrewxue98/ThermoMPNN/main/weights/v_48_020.pt\"')\n",
        "\n",
        "    from thermompnn.protein_mpnn_utils import alt_parse_PDB\n",
        "    from thermompnn import Mutation, ALPHABET\n",
        "    from thermompnn.analysis.thermompnn_benchmarking import load_model\n",
        "\n",
        "    #generate sequences and mappings\n",
        "    thermompnn_outputs = output.copy()\n",
        "\n",
        "    thermompnn_seqs = {structure: alt_parse_PDB(pdbs[structure], chains[structure])[0]['seq'] for structure in pdbs}\n",
        "\n",
        "    aligner = setup_aligner()\n",
        "    thermompnn_alignments = {pdb: aligner.align(con_seq, seq)[0] for pdb, seq in thermompnn_seqs.items()}\n",
        "\n",
        "    thermompnn_mappings = {\n",
        "        pdb: alignment_to_mapping(alignment) for pdb, alignment in thermompnn_alignments.items()\n",
        "    }\n",
        "\n",
        "    print(f'Loading ThermoMPNN model...')\n",
        "    model = load_model(\"v_48_020.pt\", \"thermoMPNN_default.pt\")\n",
        "    model = model.eval()\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    if device.type == \"cpu\":\n",
        "        print(\"WARNING: GPU not available. ThermoMPNN requires GPU for optimal performance.\")\n",
        "        print(\"Consider using a GPU-enabled runtime or skipping ThermoMPNN.\")\n",
        "    model = model.to(device)\n",
        "    print(f'ThermoMPNN model loaded.\\n')\n",
        "\n",
        "    for structure in pdbs:\n",
        "        thermompnn_scores = []\n",
        "        print(f'Running ThermoMPNN on {structure}...')\n",
        "\n",
        "        pdb_fp = pdbs[structure]\n",
        "        chain = chains[structure]\n",
        "\n",
        "        mut_pdb = alt_parse_PDB(pdb_fp, chain)\n",
        "        mutation_list = []\n",
        "\n",
        "        #try to map mutants that we previously generated to thermompnn mutants (should be identical, but might slightly mismatch due to structure handling)\n",
        "        for mut in thermompnn_outputs['mutant']:\n",
        "            if mut is None:\n",
        "                mutation_list.append(None)\n",
        "            else: #only accepts single mutants, custom uploads are skipped\n",
        "                wtAA, position, mutAA = str(mut[0]), int(str(mut[1:-1])), str(mut[-1])\n",
        "                position = position - 1 #convert from 1-indexed to zero indexed\n",
        "\n",
        "                if position in thermompnn_mappings[structure]:\n",
        "                    position = thermompnn_mappings[structure][position]\n",
        "\n",
        "                    assert (\n",
        "                        wtAA in ALPHABET\n",
        "                    ), f\"Wild type residue {wtAA} invalid, please try again with one of the following options: {ALPHABET}\"\n",
        "                    assert (\n",
        "                        mutAA in ALPHABET\n",
        "                    ), f\"Wild type residue {mutAA} invalid, please try again with one of the following options: {ALPHABET}\"\n",
        "\n",
        "                    #create thermompnn mutation objects\n",
        "                    mutation_obj = Mutation(\n",
        "                        position=position,\n",
        "                        wildtype=wtAA,\n",
        "                        mutation=mutAA,\n",
        "                        ddG=None,\n",
        "                        pdb=mut_pdb[0][\"name\"],\n",
        "                    )\n",
        "                    mutation_list.append(mutation_obj)\n",
        "                else:\n",
        "                    mutation_list.append(None)\n",
        "\n",
        "        pred, _ = model(mut_pdb, mutation_list)\n",
        "\n",
        "        for mut, out in zip(mutation_list, pred):\n",
        "            if mut is not None:\n",
        "                thermompnn_scores.append(-1 * out[\"ddG\"].cpu().item())\n",
        "            else:\n",
        "                thermompnn_scores.append(None)\n",
        "\n",
        "        thermompnn_outputs[f\"thermompnn_{structure}\"] = thermompnn_scores\n",
        "\n",
        "    #add columns to output\n",
        "    add_scaled_outputs(thermompnn_outputs, \"thermompnn\")\n",
        "    model_outputs['thermompnn'] = thermompnn_outputs\n",
        "    print('ThermoMPNN evaluation complete.')\n",
        "\n",
        "if run_esmif1 == 'yes':\n",
        "    try:\n",
        "        import esm\n",
        "    except:\n",
        "        os.system(\"pip -q install git+https://github.com/andrewxue98/esm.git\")\n",
        "        os.system(\"pip -q install torch_geometric biotite\")\n",
        "\n",
        "    import torch\n",
        "    import esm\n",
        "\n",
        "    from esm.inverse_folding.util import CoordBatchConverter\n",
        "    from scipy.special import softmax\n",
        "\n",
        "    def get_esmif_aaprobs(model, alphabet, coords, seq):\n",
        "        \"\"\"\n",
        "        Compute amino acid probabilities for a given sequence and structure using ESM-IF1.\n",
        "\n",
        "        Args:\n",
        "            model: The ESM-IF1 model.\n",
        "            alphabet: The ESM alphabet object.\n",
        "            coords: Numpy array of backbone coordinates for the structure.\n",
        "            seq: Amino acid sequence (string).\n",
        "\n",
        "        Returns:\n",
        "            aa_probs: DataFrame of amino acid probabilities (rows: AAs, columns: positions).\n",
        "        \"\"\"\n",
        "        # Get device from model\n",
        "        device = next(model.parameters()).device\n",
        "\n",
        "        # Prepare batch for the model\n",
        "        batch_converter = CoordBatchConverter(alphabet)\n",
        "        batch = [(coords, None, seq)]\n",
        "        coords, confidence, strs, tokens, padding_mask = batch_converter(\n",
        "            batch, device=device\n",
        "        )\n",
        "\n",
        "        # Prepare input tokens for the model\n",
        "        prev_output_tokens = tokens[:, :-1].to(device)\n",
        "        target = tokens[:, 1:]\n",
        "        target_padding_mask = target == alphabet.padding_idx\n",
        "\n",
        "        # Forward pass through the model\n",
        "        logits, _ = model.forward(coords, padding_mask, confidence, prev_output_tokens)\n",
        "        logits = logits.cpu().detach().numpy().squeeze()\n",
        "\n",
        "        # Compute softmax probabilities and format as DataFrame\n",
        "        aa_probs = pd.DataFrame(\n",
        "            softmax(logits),\n",
        "            index=list(alphabet.to_dict().keys())\n",
        "        )\n",
        "        # Restrict to standard amino acids\n",
        "        aa_probs = aa_probs.loc[[aa for aa in AA_ALPHABET], :]\n",
        "        # Normalize probabilities at each position\n",
        "        aa_probs = aa_probs / aa_probs.sum(axis=0)\n",
        "\n",
        "        return aa_probs\n",
        "\n",
        "\n",
        "    def get_esmif_score(seq, model, alphabet, coords):\n",
        "        \"\"\"\n",
        "        Compute the log-probability score of a sequence given structure using ESM-IF1.\n",
        "\n",
        "        Args:\n",
        "            seq: Amino acid sequence (string).\n",
        "            model: The ESM-IF1 model.\n",
        "            alphabet: The ESM alphabet object.\n",
        "            coords: Numpy array of backbone coordinates for the structure.\n",
        "\n",
        "        Returns:\n",
        "            score: Log-probability score (float).\n",
        "        \"\"\"\n",
        "        # Get amino acid probabilities for the sequence\n",
        "        aa_probs = get_esmif_aaprobs(model, alphabet, coords, seq)\n",
        "        score = 0.0\n",
        "        # Sum log-probabilities for each residue in the sequence\n",
        "        for idx, aa in enumerate(seq):\n",
        "            score += np.log(aa_probs.loc[aa, idx])\n",
        "        return score\n",
        "\n",
        "    print(f'Loading ESM-IF1 model...')\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model, alphabet = esm.pretrained.esm_if1_gvp4_t16_142M_UR50()\n",
        "    model = model.to(device)\n",
        "    model = model.eval()\n",
        "    if device.type == \"cpu\":\n",
        "        print(\"WARNING: GPU not available. ESM-IF1 will be very slow on CPU.\")\n",
        "    print(f'ESM-IF1 model loaded.\\n')\n",
        "\n",
        "    #load coordinates and sequences through esm package\n",
        "    esm_seqs = {}\n",
        "    esm_coords = {}\n",
        "    for structure in pdbs:\n",
        "        esm_structure = esm.inverse_folding.util.load_structure(pdbs[structure], chains[structure])\n",
        "        coord, backbone_seq = esm.inverse_folding.util.extract_coords_from_structure(\n",
        "            esm_structure\n",
        "        )\n",
        "        esm_seqs[structure] = backbone_seq\n",
        "        esm_coords[structure] = coord\n",
        "\n",
        "    aligner = setup_aligner()\n",
        "    esm_alignments = {pdb: aligner.align(con_seq, seq)[0] for pdb, seq in esm_seqs.items()}\n",
        "\n",
        "    esm_mappings = {\n",
        "        pdb: alignment_to_mapping(alignment) for pdb, alignment in esm_alignments.items()\n",
        "    }\n",
        "\n",
        "    esm_outputs = output.copy()\n",
        "\n",
        "    #score sequences\n",
        "    for structure in pdbs:\n",
        "        print(f'Running ESM-IF1 on {structure}...')\n",
        "        output_scores = []\n",
        "\n",
        "        wt_seq = mapping_to_sequence(con_seq, esm_seqs[structure], esm_mappings[structure])\n",
        "        wt_score = get_esmif_score(wt_seq, model, alphabet, esm_coords[structure])\n",
        "\n",
        "        for mut_seq in tqdm(esm_outputs['sequence']):\n",
        "            mapped_seq = mapping_to_sequence(mut_seq, esm_seqs[structure], esm_mappings[structure])\n",
        "            score = get_esmif_score(mapped_seq, model, alphabet, esm_coords[structure])\n",
        "            output_scores.append(score - wt_score)\n",
        "\n",
        "        esm_outputs[\"esmif1_\" + structure] = output_scores\n",
        "\n",
        "    #add to poutputs\n",
        "    add_scaled_outputs(esm_outputs, \"esmif1\")\n",
        "    model_outputs['esmif1'] = esm_outputs\n",
        "    print('ESM-IF1 evaluation complete.')\n",
        "\n",
        "#make sure at least one model was run\n",
        "if len(model_outputs) == 0:\n",
        "    raise ValueError('No CB models were run! Please enable at least one.')\n",
        "\n",
        "#try and increment run counter - skip if fails\n",
        "try:\n",
        "    increment_cb_run_count()\n",
        "except:\n",
        "    pass\n",
        "\n",
        "# Merge all model outputs\n",
        "print(\"Merging results from all models...\")\n",
        "\n",
        "multimodel_output = output.copy()\n",
        "for model in model_outputs:\n",
        "    multimodel_output = pd.merge(multimodel_output, model_outputs[model], on=['id', 'mutant', 'sequence'], how='left')\n",
        "\n",
        "display(HTML(f\"<h3>CB Output Predictions</h3>\"))\n",
        "# Display summary statistics\n",
        "print(\"\\nFinal CB Output Summary:\")\n",
        "print(f\"• Total variants analyzed: {len(multimodel_output):,}\")\n",
        "print(f\"• Single-point mutations: {len(multimodel_output[multimodel_output['id'].str.startswith('SM-')]):,}\")\n",
        "print(f\"• Custom sequences: {len(multimodel_output[multimodel_output['id'].str.startswith('CU-')]):,}\")\n",
        "\n",
        "# Show completion status for each model\n",
        "print(\"\\nModel Coverage:\")\n",
        "model_labels = {\n",
        "    'proteinmpnn': 'ProteinMPNN',\n",
        "    'frame2seq': 'Frame2Seq',\n",
        "    'thermompnn': 'ThermoMPNN',\n",
        "    'esmif1': 'ESM-IF1',\n",
        "    'ensemble': 'Ensemble'\n",
        "}\n",
        "model_cols = {\n",
        "    'proteinmpnn': ['proteinmpnn_state1', 'proteinmpnn_state2'],\n",
        "    'frame2seq': ['frame2seq_state1', 'frame2seq_state2'],\n",
        "    'thermompnn': ['thermompnn_state1', 'thermompnn_state2'],\n",
        "    'esmif1': ['esmif1_state1', 'esmif1_state2']\n",
        "}\n",
        "\n",
        "for model in model_outputs:\n",
        "    cols = model_cols[model]\n",
        "    completed = multimodel_output[cols].notna().all(axis=1).sum()\n",
        "    pct = (completed / len(multimodel_output)) * 100\n",
        "    print(f\"• {model_labels[model]} {completed:>6,}/{len(multimodel_output):<6,} sequences scored ({pct:>5.1f}%)\")\n",
        "\n",
        "#make ensemble predicti+ons across model\n",
        "multimodel_output['ensemble_state1_scaled'] = multimodel_output[[f'{x}_state1_scaled' for x in model_outputs]].mean(axis=1)\n",
        "multimodel_output['ensemble_state2_scaled'] = multimodel_output[[f'{x}_state2_scaled' for x in model_outputs]].mean(axis=1)\n",
        "\n",
        "multimodel_output['ensemble_state1_bias'] = multimodel_output['ensemble_state1_scaled'] - multimodel_output['ensemble_state2_scaled']\n",
        "multimodel_output['ensemble_state2_bias'] = multimodel_output['ensemble_state2_scaled'] - multimodel_output['ensemble_state1_scaled']\n",
        "\n",
        "all_models = (list(model_outputs.keys())  + ['ensemble'])\n",
        "\n",
        "for model in all_models:\n",
        "    multimodel_output_copy = multimodel_output.copy().dropna(subset = [f'{model}_state1_bias']).sort_values(by = f'{model}_state1_bias', ascending = False)\n",
        "    passing_mutants = multimodel_output_copy[(multimodel_output_copy[f'{model}_state1_scaled'] > 0) | (multimodel_output_copy[f'{model}_state2_scaled'] > 0)]\n",
        "    nonpassing = multimodel_output_copy[~((multimodel_output_copy[f'{model}_state1_scaled'] > 0) | (multimodel_output_copy[f'{model}_state2_scaled'] > 0))]\n",
        "\n",
        "    n_mutants_passing_filter = len(multimodel_output_copy[(multimodel_output_copy[f'{model}_state1_scaled'] > 0) | (multimodel_output_copy[f'{model}_state2_scaled'] > 0)])\n",
        "    n_biased = round(0.025 * n_mutants_passing_filter)\n",
        "\n",
        "    state1_biased, neutral, state2_biased = passing_mutants[:n_biased], passing_mutants[n_biased:-n_biased], passing_mutants[-n_biased:]\n",
        "\n",
        "    s1_set, s2_set, neutral_set, nonpassing_set = set(state1_biased['mutant']), set(state2_biased['mutant']), set(neutral['mutant']), set(nonpassing['mutant'])\n",
        "\n",
        "    assignments = []\n",
        "    for m in multimodel_output['mutant']:\n",
        "        if m in set(state1_biased['mutant']):\n",
        "            assignment = 'state1'\n",
        "        elif m in set(state2_biased['mutant']):\n",
        "            assignment = 'state2'\n",
        "        elif m in neutral_set:\n",
        "            assignment = 'neutral'\n",
        "        elif m in set(nonpassing['mutant']):\n",
        "            assignment = 'low'\n",
        "        else:\n",
        "            assignment = None\n",
        "\n",
        "        assignments.append(assignment)\n",
        "\n",
        "    multimodel_output[f'{model}_assignment'] = assignments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "36VMYHZ8eOnj"
      },
      "outputs": [],
      "source": [
        "#@title CB Results & Analysis\n",
        "#@markdown This code block will generate a CB plot (scatter plot of scaled State 1 vs. State 2 scores) and label bias predicted mutants.\n",
        "#@markdown If state 1 and state 2 scores are highly correlated, it may be worth to check if the structures you uploaded are meaningfully different.\n",
        "#@markdown Ensemble will give scaled scores averaged over all models run and may help to smooth out model-specific biases.\n",
        "\n",
        "model_to_analyze = \"proteinmpnn\" # @param [\"proteinmpnn\",\"frame2seq\",\"thermompnn\",\"esmif1\", \"ensemble\"]\n",
        "\n",
        "cmap = {'state1': 'red', 'state2': 'blue', 'neutral': 'grey', 'low': 'grey'}\n",
        "\n",
        "if model_to_analyze in all_models:\n",
        "    passing = multimodel_output[multimodel_output[f'{model_to_analyze}_assignment'] != 'low']\n",
        "    nonpassing = multimodel_output[multimodel_output[f'{model_to_analyze}_assignment'] == 'low']\n",
        "\n",
        "    state1_cutoff = multimodel_output[multimodel_output[f'{model_to_analyze}_assignment'] == 'state1'][f'{model_to_analyze}_state1_bias'].min()\n",
        "    state2_cutoff = multimodel_output[multimodel_output[f'{model_to_analyze}_assignment'] == 'state2'][f'{model_to_analyze}_state2_bias'].min()\n",
        "\n",
        "    plt.figure(figsize = (10,10))\n",
        "    plt.title('Conformational Design Mutants (Top 5% mutants)')\n",
        "\n",
        "    plt.scatter(passing[f'{model_to_analyze}_state1_scaled'], passing[f'{model_to_analyze}_state2_scaled'], marker = 'o', alpha = 0.7, edgecolor = 'black', c=[cmap[x] for x in passing[f'{model_to_analyze}_assignment']])\n",
        "    plt.scatter(nonpassing[f'{model_to_analyze}_state1_scaled'], nonpassing[f'{model_to_analyze}_state2_scaled'], marker = 'o', alpha = 0.25, edgecolor = 'black', c=[cmap[x] for x in nonpassing[f'{model_to_analyze}_assignment']])\n",
        "\n",
        "    #set limits to be equal on both axes\n",
        "    xmin, xmax = plt.xlim()\n",
        "    ymin, ymax = plt.ylim()\n",
        "\n",
        "    umin, umax = min(xmin, ymin), max(xmax, ymax)\n",
        "    plt.xlim(umin, umax)\n",
        "    plt.ylim(umin, umax)\n",
        "\n",
        "    plt.plot([umin, 0], [0,0], color = 'black')\n",
        "    plt.plot([0, 0], [umin,0], color = 'black')\n",
        "\n",
        "    plt.plot([-state2_cutoff, umax-state2_cutoff], [0, umax], color = 'black')\n",
        "    plt.plot([0, umax], [-state1_cutoff, umax -state1_cutoff], color = 'black')\n",
        "\n",
        "    plt.xlabel(f'State 1 {model_labels[model_to_analyze]} Score')\n",
        "    plt.ylabel(f'State 2 {model_labels[model_to_analyze]} Score')\n",
        "\n",
        "    text_offset = 0.1\n",
        "    plt.text(umax - text_offset, umax - text_offset, 'Neutral Mutants', horizontalalignment = 'right', verticalalignment = 'top')\n",
        "    plt.text(umax - text_offset, umin + text_offset, 'State 1 Bias Predicted Mutants', horizontalalignment = 'right', verticalalignment = 'bottom')\n",
        "    plt.text(umin + text_offset, umax - text_offset, 'State 2 Bias Predicted Mutants', horizontalalignment = 'left', verticalalignment = 'top')\n",
        "    plt.text(umin + text_offset, umin + text_offset, 'Low Scoring Mutants', horizontalalignment = 'left', verticalalignment = 'bottom')\n",
        "else:\n",
        "    display(HTML(f\"\"\"<h4><font color='red'>Results from model {model_to_analyze} were not found!</font></h4>\"\"\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "uCoMdB8GpO2A"
      },
      "outputs": [],
      "source": [
        "#@title Export Results\n",
        "#@markdown Exports scored variants as a csv, including bias predictions.\n",
        "from datetime import date\n",
        "\n",
        "today = date.today()\n",
        "\n",
        "output_filepath = f'{jobname}_{today.month}{today.day}{today.year}_cb_variants.csv'\n",
        "multimodel_output.to_csv(output_filepath, index = False)\n",
        "files.download(output_filepath)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}